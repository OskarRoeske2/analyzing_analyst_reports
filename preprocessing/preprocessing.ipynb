{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ace_tools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.5.4)\n",
      "Requirement already satisfied: tabula in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.10.19)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (75.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ace_tools spacy tabula https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Relevant Files and models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "provider_info = pd.read_csv('provider_info.csv')\n",
    "company_info = pd.read_csv('company_info.csv')\n",
    "\n",
    "company_info = company_info.drop_duplicates(subset='Ticker Symbol')\n",
    "\n",
    "# Create a dictionary to map ticker symbols to company name and industry\n",
    "ticker_map = company_info.set_index('Ticker Symbol')[['Company Name', 'Industry']].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Only relevant to check font type and size for\\nimport pdfplumber\\n\\ndef extract_words_with_formatting(page):\\n\\n    # Extract words with their bounding boxes\\n    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\\n\\n    formatted_words = []\\n    for word in words:\\n        formatted_words.append({\\n            \"word\": word[\"text\"],\\n            \"font\": word.get(\"fontname\", \"Unknown\"),\\n            \"size\": word.get(\"size\", \"Unknown\"),\\n            \"x0\": word[\"x0\"],\\n            \"x1\": word[\"x1\"],\\n            \"top\": word[\"top\"],\\n            \"bottom\": word[\"bottom\"]\\n        })\\n    return formatted_words\\n\\n\\n# Example usage with pdfplumber\\npdf_path = \"/Users/oskarroeske/Desktop/Analyst_Reports/Production/20140116_Brean_Capital_SIG_SIG-_Some_Pyrite_Mixed_In_Among_the_Diamonds-_Tweaking.pdf\"\\n\\nwith pdfplumber.open(pdf_path) as pdf:\\n    for page_number, page in enumerate(pdf.pages, start=1):\\n        print(f\"Page {page_number}:\")\\n        formatted_words = extract_words_with_formatting(page)\\n        for word_info in formatted_words:\\n            print(\\n                f\"Word: \\'{word_info[\\'word\\']}\\', Font: {word_info[\\'font\\']}, Size: {word_info[\\'size\\']}, \"\\n                f\"Position: ({word_info[\\'x0\\']}, {word_info[\\'top\\']} - {word_info[\\'x1\\']}, {word_info[\\'bottom\\']})\"\\n            )'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Only relevant to check font type and size for\n",
    "import pdfplumber\n",
    "\n",
    "def extract_words_with_formatting(page):\n",
    "\n",
    "    # Extract words with their bounding boxes\n",
    "    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\n",
    "\n",
    "    formatted_words = []\n",
    "    for word in words:\n",
    "        formatted_words.append({\n",
    "            \"word\": word[\"text\"],\n",
    "            \"font\": word.get(\"fontname\", \"Unknown\"),\n",
    "            \"size\": word.get(\"size\", \"Unknown\"),\n",
    "            \"x0\": word[\"x0\"],\n",
    "            \"x1\": word[\"x1\"],\n",
    "            \"top\": word[\"top\"],\n",
    "            \"bottom\": word[\"bottom\"]\n",
    "        })\n",
    "    return formatted_words\n",
    "\n",
    "\n",
    "# Example usage with pdfplumber\n",
    "pdf_path = \"/Users/oskarroeske/Desktop/Analyst_Reports/Production/20140116_Brean_Capital_SIG_SIG-_Some_Pyrite_Mixed_In_Among_the_Diamonds-_Tweaking.pdf\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        print(f\"Page {page_number}:\")\n",
    "        formatted_words = extract_words_with_formatting(page)\n",
    "        for word_info in formatted_words:\n",
    "            print(\n",
    "                f\"Word: '{word_info['word']}', Font: {word_info['font']}, Size: {word_info['size']}, \"\n",
    "                f\"Position: ({word_info['x0']}, {word_info['top']} - {word_info['x1']}, {word_info['bottom']})\"\n",
    "            )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patterns per provider for target price, ending (disclosures), rating, font type and size\n",
    "\n",
    "patterns = {\n",
    "    \"APP Securities Pty Ltd\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"TARGET PRICE (NA|A\\$(\\d+(\\.\\d+)?))\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?\", \"font_size\": 8.25},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Alliance Global Partners\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target (NA|\\$(\\d+(\\.\\d+)?))\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Imporant Research Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]ArialMT(-BoldMT)?\", \"font_size\": 8.00},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Barclays\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target USD (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price Target: USD (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Stock Rating ([A-Za-z]+)\",\n",
    "            \"secondary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|OVERPERFORM|UNDERPERFORM|UNDERWEIGHT|NEUTRAL)\\b\",\n",
    "            \"tertiary\": r\"\\b(Buy|Hold|Sell|Overweight|Overperform|Underperform|Underweight|Neutral)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"ANALYST\\(S\\) CERTIFICATION\\(S\\)\",\n",
    "                            r\"Analyst\\(s\\) Certification\\(s\\)\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Expert Sans (Extra Bold)?(Regular)?(Regular,Bold)?\", \"font_size\": 9.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Expert Sans (Extra Bold)?(Regular)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]SourceSansPro(-Bold)?(-Regular)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 9.01},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.58},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.52},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 8.52},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 8.75},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.60},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.11},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 6.52},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.02},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 7.03},\n",
    "        ]\n",
    "    },\n",
    "    \"BGC Partners\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\(\\$\\) (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+) \\(\\w+,\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures Appendix\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 10.08},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Brean Capital\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"PT: \\$ (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"TP: \\$(\\d+(\\.\\d+)?)\",\n",
    "\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform)\\b\"\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\",r\"Important Disclosures \"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?(-Italic)?\", \"font_size\": 8.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?(-Italic)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"BTIG\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\$(\\d+(\\.\\d+)?) 12 month target \",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix: Analyst Certification and Other Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Corbel(,Bold)?(,-Italic)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Corbel(,Bold)?(,-Italic)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?(,Italic)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?(-Italic)?\", \"font_size\": 10.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?(,Italic)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Cascend Securities -Historical-\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price target: \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rating: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures: \"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?\", \"font_size\": 12.0},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Deutsche Bank\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\(USD\\) (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price target (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform|Underweight|Neutral)\\b\",\n",
    "            #\"secondary\": r\"Rating ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix 1\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]UniversDeutscheBank-Regular\", \"font_size\": 9.0},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"FinTrust Investment Advisors\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Target Price: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Fintrust Rating: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Important Disclosures:\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Arial(-BoldMT)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Gilford Securities Inc\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\, \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rated: ([A-Za-z]+)\",\n",
    "            \"secondary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "            \"tertiary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform|Underweight|Neutral)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"ANALYST CERTIFICATION\",r\"REQUIRED DISCLOSURES\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\":  r\"Arial(MT)?(-BoldMT)?\", \"font_size\": 10.02},\n",
    "            {\"font_type\":  r\"Arial(MT)?(-BoldMT)?\", \"font_size\": 10.01},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.00},\n",
    "        ]\n",
    "    },\n",
    "    \"Hilliard Lyons\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target (NA|\\$(\\d+(\\.\\d+)?))\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"-- NYSE\\s+[–\\-—]+\\s+([A-Za-z]+)\\s+[–\\-—]+\",\n",
    "            \"secondary\":r\"NYSE\\s+[–\\-—]+\\s+([A-Za-z\\- ]+?)(?=\\s*[-–—]\\d)\"\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification I,\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Verdana(-Bold)?(-Italic)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"Verdana(-Bold)?(-Italic)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\":r\"TimesNewRomanPS(-BoldMT)?\", \"font_size\": 10.98},\n",
    "            {\"font_type\":r\"Times(-Bold)?\", \"font_size\": 10.98},\n",
    "            ]\n",
    "    },\n",
    "    \"IBI Investment House\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price target: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Recommendation: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"IFS Securities\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"(Buy|Hold|Sell|Overweight|Underperform|Outperform)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Important Investor Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Arial(-BoldMT)?\", \"font_size\": 11.04},\n",
    "            {\"font_type\":r\"TimesNewRomanPS(MT)?(-BoldMT)?\", \"font_size\": 11.04},\n",
    "        ]\n",
    "    },\n",
    "    \"JP Morgan\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price Target \\([A-Za-z0-9\\-]+\\): \\$(\\d+(\\.\\d+)?)\"\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform|Underweight|Neutral)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification:\",\"Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRoman(,Bold)?(,Italic)?\", \"font_size\": 9.60},\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRoman(,Bold)?(,Italic)?\", \"font_size\": 10.08},\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRomanPS(MT)?(-BoldMT)?(-ItalicMT)?\", \"font_size\": 11.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRomanPS(MT)?(-BoldMT)?(-ItalicMT)?\", \"font_size\": 10.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRoman(,Bold)?(,Italic)?\", \"font_size\": 10.56},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Mizuho Securities\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rating ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"IMPORTANT DISCLOSURES\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": r\"Times-Roman(-Bold)?(-Italic)?(-BoldItalic)?\", \"font_size\": 10.50},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Needham\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"PRICE TARGET: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"tertiary\": r\"Price Target \\$(\\d+(\\.\\d+)?)\",\n",
    "\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "            \"secondary\": r\"Rating (\\w+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"\n",
    "                            r\"ANALYST CERTIFICATION\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Cambria(-Bold)?(-Regular)?\", \"font_size\": 10.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Cambria(-Bold)?(-Regular)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]KeplerStd(-Bold)?\", \"font_size\": 9.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Phillip Securities\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"TARGET PRICE USD (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"U\\s?S\\s?D\\s?(\\d+(?:\\s?\\.\\s?\\d+)?)\\s?T\\s?A\\s?R\\s?G\\s?E\\s?T\\s?P\\s?R\\s?I\\s?C\\s?E\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "            \"secondary\": r\"\\b(B U Y|H O L D|S E L L|O V E R W E I G H T|U N D E R P E R F O R M| N E U T R A L | O U T P E R F O R M)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Contact Information\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?\", \"font_size\": 10.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?\", \"font_size\": 9.99},\n",
    "        ]\n",
    "    },\n",
    "    \"Pivotal Research Group\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Target Price: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"RATING: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix: Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Helvetica(-Bold)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Helvetica(-Bold)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Helvetica(-Bold)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Helvetica(-Bold)?\", \"font_size\": 8.99},\n",
    "            {\"font_type\": r\"Arial(MT)?(-BoldMT)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"Arial(MT)?(-BoldMT)?\", \"font_size\": 8.03},\n",
    "            {\"font_type\": r\"Arial(MT)?(-BoldMT)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Arial\", \"font_size\": 9.00},\n",
    "        ]\n",
    "    },\n",
    "    \"Spartan Capital Securities LLC\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"T \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Wells Fargo\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\/Price Target: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price Target\\/Prior: \\$(\\d+(\\.\\d+)?)\",\n",
    "            #\"tertiary\": r\"\\/\\$(\\d+(\\.\\d+)?)\"\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\/\\b(Buy|Hold|Sell|Overweight|Underperform|Underweight|Neutral)\\b\",\n",
    "            \"secondary\": r\"Rating (\\w+)\",\n",
    "            \"tertiary\": r\"([A-Za-z]+)/\\$\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Required Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]WellsFargoSans(-Light)?(,-SemiBold)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Verdana(-Bold)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Verdana(-Bold)?\", \"font_size\": 9.88},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 9.01}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(paragraph):\n",
    "    # Parse the paragraph\n",
    "    paragraph = re.sub(r\"\\s+\", \" \", paragraph).strip()\n",
    "    doc = nlp(paragraph)\n",
    "        \n",
    "    for sent in doc.sents:\n",
    "        has_verb = False\n",
    "        has_subject = False\n",
    "        \n",
    "        for token in sent:\n",
    "            # Check for a verb\n",
    "            if token.pos_ in {\"VERB\", \"AUX\"}:\n",
    "                has_verb = True\n",
    "            # Check for a subject\n",
    "            if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                has_subject = True\n",
    "        \n",
    "        # If both a verb and a subject are found, the sentence is valid\n",
    "        if has_verb and has_subject:\n",
    "            return True\n",
    "        \n",
    "        # At least one word with 5+ letters and all upper case -> probably a headline (will also be included)\n",
    "        if re.search(r\"[A-Z]{5,}\", paragraph):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_valid_paragraphs(paragraphs):\n",
    "    valid_paragraphs = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if check_validity(paragraph):\n",
    "            valid_paragraphs.append(paragraph)\n",
    "    return valid_paragraphs\n",
    "\n",
    "# Function to check if a word ends a sentence\n",
    "def is_sentence_end(word):\n",
    "    text = word[\"text\"]\n",
    "    next_text = word.get(\"next_text\", \"\")\n",
    "\n",
    "    # Sentences definitely end with ! or ?, for . has to be checked further\n",
    "    if text.endswith(\".\") or text.endswith(\"!\") or text.endswith(\"?\"):\n",
    "        # Ensure it's not part of a decimal number\n",
    "        if next_text:\n",
    "            if text.replace(\".\", \"\").isdigit() and next_text.isdigit():\n",
    "                return False\n",
    "            # Check if the next word starts with an uppercase letter (for \".\")\n",
    "            if text.endswith(\".\") and next_text and not next_text.istitle():\n",
    "                return False\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_text_with_format(page, provider, page_number,url_date):\n",
    "\n",
    "    if isinstance(url_date, datetime):\n",
    "        url_date = url_date.date()\n",
    "\n",
    "    date = None\n",
    "\n",
    "    # Access patterns\n",
    "    provider_patterns = patterns[provider]\n",
    "    price_patterns = provider_patterns[\"price_patterns\"]\n",
    "    rating_patterns = provider_patterns[\"rating_patterns\"]\n",
    "    ending_patterns = provider_patterns[\"ending_patterns\"]\n",
    "    font_patterns = provider_patterns[\"font_patterns\"]\n",
    "\n",
    "    # Extract words with font and size details\n",
    "    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\n",
    "\n",
    "    # Handle empty words\n",
    "    if not words:\n",
    "        return [], False, None, None, date\n",
    "\n",
    "    # Round text sizes to 2 decimal places -> for calculation\n",
    "    for word in words:\n",
    "        if \"size\" in word and word[\"size\"] is not None:\n",
    "            word[\"size\"] = round(word[\"size\"], 2)\n",
    "\n",
    "    # Sort words by vertical and horizontal position\n",
    "    words.sort(key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "    current_top = None\n",
    "    lookahead_buffer = []\n",
    "    rating = None\n",
    "    price = None\n",
    "    extracted_date = None\n",
    "\n",
    "    # Add next word context for sentence-ending logic\n",
    "    for i in range(len(words) - 1):\n",
    "        words[i][\"next_text\"] = words[i + 1][\"text\"]\n",
    "    words[-1][\"next_text\"] = None \n",
    "\n",
    "\n",
    "    # Check all words of the document\n",
    "    for word in words:\n",
    "        \n",
    "        # Build lookahead buffer\n",
    "        lookahead_buffer.append(word[\"text\"])\n",
    "        if len(lookahead_buffer) > 35:\n",
    "            lookahead_buffer.pop(0)\n",
    "        buffer_text = \" \".join(lookahead_buffer)\n",
    "\n",
    "        # Extract rating or price (only for page 1) -> more efficient, because rating etc. not relevant\n",
    "        if page_number == 1:\n",
    "            # Extract rating\n",
    "            if not rating:\n",
    "                for pattern_key in [\"primary\", \"secondary\", \"tertiary\"]:\n",
    "                    pattern = rating_patterns.get(pattern_key)\n",
    "                    if pattern:\n",
    "                        rating_match = re.search(pattern, buffer_text)\n",
    "                        if rating_match:\n",
    "                            rating = rating_match.group(1)\n",
    "                            break\n",
    "\n",
    "            # Extract price\n",
    "            if not price:\n",
    "                for pattern_key in [\"primary\", \"secondary\", \"tertiary\"]:\n",
    "                    pattern = price_patterns.get(pattern_key)\n",
    "                    if pattern:\n",
    "                        price_match = re.search(pattern, buffer_text)\n",
    "                        if price_match:\n",
    "                            price = price_match.group(1)\n",
    "                            break\n",
    "\n",
    "            # Look for date entities in the text\n",
    "            if len(buffer_text.split()) > 30:\n",
    "                if date is None:\n",
    "                    sequence = nlp(buffer_text)\n",
    "                    for ent in sequence.ents:\n",
    "                        if ent.label_ == \"DATE\":\n",
    "                            try:\n",
    "                                candidate_date = parse(ent.text, fuzzy=True).date()\n",
    "                        \n",
    "                                # Check if date is date is valid\n",
    "                                if datetime.min.date() <= candidate_date <= datetime.max.date():\n",
    "                                    extracted_date = candidate_date\n",
    "                                    break  # Stop if date is found\n",
    "\n",
    "                            except (ValueError, OverflowError):\n",
    "                                continue  # Skip invalid dates\n",
    "                    \n",
    "                    # Calculate the difference\n",
    "                    date_difference = 0\n",
    "                    if extracted_date:\n",
    "                        date_difference = (url_date - extracted_date).days\n",
    "                        \n",
    "                    # Check the conditions (should only be taken if within 10 days)\n",
    "                    if extracted_date and (0 <= date_difference <= 10):\n",
    "                        date = extracted_date\n",
    "                        \n",
    "        # Check for ending pattern\n",
    "        for ending_pattern in ending_patterns:\n",
    "            if re.search(ending_pattern, buffer_text):\n",
    "                return filter_valid_paragraphs(paragraphs), True, rating, price, date\n",
    "        \n",
    "        # Match word against font patterns, include only if both font and type are matched\n",
    "        is_font_matched = False\n",
    "        for font_pattern in font_patterns:\n",
    "            font_type = font_pattern[\"font_type\"]\n",
    "            font_size = font_pattern[\"font_size\"]\n",
    "            if re.match(font_type, word[\"fontname\"]) and word[\"size\"] == font_size:\n",
    "                is_font_matched = True\n",
    "                break\n",
    "\n",
    "        if not is_font_matched:\n",
    "            continue\n",
    "\n",
    "        # Check if we need to start a new paragraph, vertical distance relevant\n",
    "        if current_top is not None and abs(word[\"top\"] - current_top) >= 13:\n",
    "            if word['text'] and word['text'][0].islower():\n",
    "                # add word to current paragraph\n",
    "                current_paragraph.append(word[\"text\"])\n",
    "            else:\n",
    "                #end paragraph and start a new one\n",
    "                paragraphs.append(\" \".join(current_paragraph))\n",
    "                current_paragraph = [word[\"text\"]]\n",
    "        else:\n",
    "            # Continue the current paragraph\n",
    "            current_paragraph.append(word[\"text\"])\n",
    "\n",
    "\n",
    "        current_top = word[\"top\"]\n",
    "\n",
    "    #Add paragraph to list of paragraphs\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    return filter_valid_paragraphs(paragraphs), False, rating, price, date\n",
    "\n",
    "\n",
    "def extract_metadata(filename, ticker_map):\n",
    "    \n",
    "    # Extract the date (first 8 digits in the filename)\n",
    "    date_match = re.match(r\"(\\d{8})\", filename)\n",
    "    if not date_match:\n",
    "        return None, None, None, None, None\n",
    "    date_str = date_match.group(1)\n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "\n",
    "    # Look for the ticker in the filename\n",
    "    for ticker in ticker_map.keys():\n",
    "        ticker_pattern = f\"_{ticker}_\"  # Ensure ticker is surrounded by underscores\n",
    "        if ticker_pattern in filename:\n",
    "            # Extract the portion between date and ticker as the provider\n",
    "            provider_section = filename.split(f\"{date_str}_\")[1].split(f\"_{ticker}_\")[0]\n",
    "            provider = provider_section.replace('_', ' ')  # Replace underscores with spaces\n",
    "            # Get company name and industry from the ticker_map\n",
    "            company_name = ticker_map[ticker]['Company Name']\n",
    "            industry = ticker_map[ticker]['Industry']\n",
    "            return date, provider, ticker, company_name, industry\n",
    "\n",
    "    # If no ticker is found, return None for ticker-related fields\n",
    "    return date, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname E identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "# Define function to process all PDFs in the directory and store data in DataFrame\n",
    "def extract_text_from_all_pdfs_to_dataframe(directory_path, ticker_map):\n",
    "    data = []\n",
    "    id_counter = 1  # Initialize an ID counter\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Extract metadata from filename\n",
    "            url_date, provider, ticker, company_name, industry = extract_metadata(filename,ticker_map=ticker_map)\n",
    "            all_paragraphs = []  \n",
    "            first_rating = None\n",
    "            first_price = None\n",
    "            first_date = None\n",
    "            stop_extraction = False\n",
    "\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                    if stop_extraction:\n",
    "                        break  # Exit the loop if stop_extraction is set\n",
    "            \n",
    "                    # Extract data from the current page\n",
    "                    paragraphs, stop_extraction, rating, price,new_date = extract_text_with_format(page, provider=provider,page_number=page_number,url_date=url_date)\n",
    "                    \n",
    "                    # Append paragraphs from the current page\n",
    "                    all_paragraphs.extend(paragraphs)\n",
    "\n",
    "                    # Capture the first non-None rating and price\n",
    "                    if rating is not None and first_rating is None:\n",
    "                        first_rating = rating\n",
    "                    if price is not None and first_price is None:\n",
    "                        first_price = price\n",
    "\n",
    "                    if new_date is not None and first_date is None:\n",
    "                        first_date = new_date\n",
    "                        \n",
    "            if first_date is None:\n",
    "                first_date = url_date.date()\n",
    "                        \n",
    "            # Add extracted data to the list\n",
    "            data.append({\n",
    "                \"document_id\": id_counter,  # Unique ID\n",
    "                \"filename\": filename,\n",
    "                \"date\": first_date,\n",
    "                \"provider\": provider,\n",
    "                \"ticker\": ticker,\n",
    "                \"company_name\": company_name,\n",
    "                \"industry\": industry,\n",
    "                \"paragraphs\": all_paragraphs,\n",
    "                \"target_price\":first_price.replace(\" \",\"\") if first_price else None,\n",
    "                \"rating\": first_rating.lower().replace(\" \",\"\") if first_rating else None\n",
    "                })\n",
    "            id_counter += 1  # Increment the ID counter for the next row\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Directory path\n",
    "pdf_directory = \"/Users/oskarroeske/Desktop/Analyst_Reports/Production\"\n",
    "\n",
    "# Run the function and store results in a DataFrame\n",
    "df_reports = extract_text_from_all_pdfs_to_dataframe(pdf_directory, ticker_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_paragraph(paragraph):\n",
    "    # Remove email addresses\n",
    "    paragraph = re.sub(r\"\\S+@\\S+\", \"\", paragraph)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    paragraph = re.sub(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}(?:\\s?(?:ext|x|ext.)\\s?\\d{1,5})?\\b\", \"\", paragraph)\n",
    "    \n",
    "    # Remove URLs\n",
    "    paragraph = re.sub(r\"http\\S+|www\\S+\", \"\", paragraph)\n",
    "    \n",
    "    # REmove special characters (keep relevant sign for financial documents)\n",
    "    paragraph = re.sub(r\"[^/\\w\\s,.&!?%$:-]\", \"\", paragraph)\n",
    "    \n",
    "    # Remove multiple spaces or newlines\n",
    "    paragraph = re.sub(r\"\\s+\", \" \", paragraph).strip()\n",
    "\n",
    "    #Check Headers that are in line with the other texts\n",
    "    paragraph = re.sub(r\"\\b(?:[A-Z]{2,}\\s+){2,}[A-Z]{2,}\\b\", \"\", paragraph)\n",
    "    \n",
    "    # Remove short paragraphs (fewer than 6 words) -> probably not an actual paragraph (subjectively set)\n",
    "    if len(paragraph.split()) < 6:\n",
    "        return None\n",
    "    \n",
    "    return paragraph\n",
    "\n",
    "# Clean paragaphs\n",
    "df_reports['paragraphs'] = df_reports['paragraphs'].apply(\n",
    "    lambda paragraphs: [clean_paragraph(p) for p in paragraphs if clean_paragraph(p)]\n",
    ")\n",
    "\n",
    "# Drop rows where the `paragraphs` column is empty after cleaning\n",
    "df_reports = df_reports[df_reports['paragraphs'].str.len() > 0]\n",
    "\n",
    "# Convert rating to string\n",
    "df_reports[\"rating\"] = df_reports[\"rating\"].astype(str).apply(lambda x: x.lower() if x != \"nan\" else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Date and Price for Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already prepared data from yahoo finance\n",
    "df_performance_data = pd.read_csv(\"performance_data.csv\")\n",
    "\n",
    "# Convert 'Date' column to datetime and set it as index (with timezone awareness)\n",
    "df_performance_data['Date'] = pd.to_datetime(df_performance_data['Date'], utc=True)\n",
    "df_performance_data = df_performance_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports = df_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prices(ticker, start_date, end_date=None):\n",
    "    try:\n",
    "        # Ensure start_date and end_date are in the same timezone (UTC)\n",
    "        start_date = pd.to_datetime(start_date, utc=True)\n",
    "        if end_date is not None:\n",
    "            end_date = pd.to_datetime(end_date, utc=True)\n",
    "        else:\n",
    "            end_date = start_date  # If no end_date, use start_date for a single day\n",
    "\n",
    "        # Filter the data for the ticker and date range\n",
    "        filtered_data = df_performance_data.loc[\n",
    "            (df_performance_data.index >= start_date) & \n",
    "            (df_performance_data.index <= end_date), ticker]\n",
    "\n",
    "        if not filtered_data.empty:\n",
    "            # Return the maximum & minimum price within the filtered date range -> used for accuracy for buy/sell recommendations\n",
    "            max_price = filtered_data.max()\n",
    "            min_price = filtered_data.min()\n",
    "            return round(float(max_price),2), round(float(min_price),2)\n",
    "        else:\n",
    "            # If no data available in the range, find the next available date, asof selects the next available date, after the start date\n",
    "            next_available_data = df_performance_data[ticker].asof(start_date)\n",
    "            if next_available_data is not None:\n",
    "                return round(float(next_available_data),2), round(float(next_available_data),2)\n",
    "            else:\n",
    "                return round(float('nan')), round(float('nan'))  # Return NaN for missing data\n",
    "\n",
    "    except KeyError:\n",
    "        return float('nan'), float('nan')  # Return NaN for missing data\n",
    "    except Exception as e:\n",
    "        return float('nan'), float('nan')  # Return NaN for missing data\n",
    "\n",
    "def calculate_prices(row):\n",
    "    short_name = row['ticker']\n",
    "    base_date = pd.to_datetime(row['date'], utc=True)\n",
    "\n",
    "    # Calculate prices\n",
    "    row['start price'] = get_stock_prices(short_name, base_date)[0]\n",
    "    row['one day after'] = get_stock_prices(short_name, base_date + pd.DateOffset(days=1))[0]\n",
    "    row['max price after 3 months'], row[\"min price after 3 months\"] = get_stock_prices(\n",
    "        short_name, base_date, base_date + pd.DateOffset(months=3)\n",
    "    )\n",
    "    row['max price after 6 months'], row['min price after 6 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=3), base_date + pd.DateOffset(months=6)\n",
    "    )\n",
    "    row['max price after 9 months'], row['min price after 9 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=6), base_date + pd.DateOffset(months=9)\n",
    "    )\n",
    "    row['max price after 12 months'], row['min price after 12 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=9), base_date + pd.DateOffset(months=12)\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "df_saved_reports = df_saved_reports.apply(calculate_prices, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "overweight      275\n",
       "buy             219\n",
       "hold            143\n",
       "none            101\n",
       "neutral          50\n",
       "equal            36\n",
       "underweight      33\n",
       "v                16\n",
       "underperform     11\n",
       "weight           11\n",
       "sell              7\n",
       "unchanged         6\n",
       "outperform        3\n",
       "basis             2\n",
       "long-termbuy      2\n",
       "change            1\n",
       "b                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saved_reports.value_counts(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target_price</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>150.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200807_Wells_Fargo_META_FB-_2.5B_Person_Plat...</td>\n",
       "      <td>300</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200918_Barclays_GM_General_Motors-_Time_to_t...</td>\n",
       "      <td>39.00</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200722_Barclays_BKNG_Booking_Holdings_Inc.-_...</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20211104_Deutsche_Bank_X_US_Steel-_3Q21_EBITDA...</td>\n",
       "      <td>50.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>20150522_Gilford_Securities_Inc_FL_Report_rece...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>20200526_Wells_Fargo_HD_HD-_Q2_Rollover_Falls_...</td>\n",
       "      <td>270</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>20220125_Deutsche_Bank_WFC_Wells_Fargo-_More_C...</td>\n",
       "      <td>55.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>20200212_Barclays_GM_General_Motors-_Waiting_p...</td>\n",
       "      <td>44.00</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>1070</td>\n",
       "      <td>hold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>917 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename target_price  \\\n",
       "0    20161018_Needham_META_Facebook-_3Q16_Preview_R...       150.00   \n",
       "1    20200807_Wells_Fargo_META_FB-_2.5B_Person_Plat...          300   \n",
       "2    20200918_Barclays_GM_General_Motors-_Time_to_t...        39.00   \n",
       "3    20200722_Barclays_BKNG_Booking_Holdings_Inc.-_...      1950.00   \n",
       "4    20211104_Deutsche_Bank_X_US_Steel-_3Q21_EBITDA...        50.00   \n",
       "..                                                 ...          ...   \n",
       "912  20150522_Gilford_Securities_Inc_FL_Report_rece...          NaN   \n",
       "913  20200526_Wells_Fargo_HD_HD-_Q2_Rollover_Falls_...          270   \n",
       "914  20220125_Deutsche_Bank_WFC_Wells_Fargo-_More_C...        55.00   \n",
       "915  20200212_Barclays_GM_General_Motors-_Waiting_p...        44.00   \n",
       "916  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...         1070   \n",
       "\n",
       "         rating  \n",
       "0           buy  \n",
       "1    overweight  \n",
       "2    overweight  \n",
       "3    overweight  \n",
       "4           buy  \n",
       "..          ...  \n",
       "912         buy  \n",
       "913  overweight  \n",
       "914         buy  \n",
       "915  overweight  \n",
       "916        hold  \n",
       "\n",
       "[917 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually updated some missing data and imported it back to the notebook for further steps\n",
    "df_updated_reports = pd.read_csv(\"updated_target_prices_ratings.csv\")\n",
    "df_updated_reports['date'] = pd.to_datetime(df_updated_reports['date'], format=\"%Y-%m-%d\")\n",
    "df_updated_reports['date'] = df_updated_reports['date'].dt.date\n",
    "\n",
    "df_updated_reports[\"paragraphs\"] = df_updated_reports[\"paragraphs\"].apply(ast.literal_eval)\n",
    "df_updated_reports = df_updated_reports[[\"filename\",\"target_price\",\"rating\"]]\n",
    "df_updated_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge manually updated data back to existing df (target prices and ratings were manually checked)\n",
    "df_merged = df_saved_reports.merge(df_updated_reports, on='filename', how='left', suffixes=('', '_new'))\n",
    "columns_to_update = ['target_price', 'rating']\n",
    "for col in columns_to_update:\n",
    "    df_merged[col] = df_merged[f'{col}_new'].combine_first(df_merged[col])\n",
    "\n",
    "df_merged = df_merged.drop(columns=[f'{col}_new' for col in columns_to_update])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust target price for companies that had a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports[\"target_price\"] = (\n",
    "    df_saved_reports[\"target_price\"]\n",
    "    .replace(\",\", \"\", regex=True)  # Remove commas\n",
    "    .pipe(pd.to_numeric, errors=\"coerce\")  # Convert to float; invalid values become NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>provider</th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>target_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>one day after</th>\n",
       "      <th>max price after 3 months</th>\n",
       "      <th>min price after 3 months</th>\n",
       "      <th>max price after 6 months</th>\n",
       "      <th>min price after 6 months</th>\n",
       "      <th>max price after 9 months</th>\n",
       "      <th>min price after 9 months</th>\n",
       "      <th>max price after 12 months</th>\n",
       "      <th>min price after 12 months</th>\n",
       "      <th>adjusted_target_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>Meta Platforms Inc.</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>[INVESTMENT HIGHLIGHTS: $150.00 We raise our e...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>buy</td>\n",
       "      <td>...</td>\n",
       "      <td>129.05</td>\n",
       "      <td>133.28</td>\n",
       "      <td>115.05</td>\n",
       "      <td>142.65</td>\n",
       "      <td>126.09</td>\n",
       "      <td>155.27</td>\n",
       "      <td>139.39</td>\n",
       "      <td>173.51</td>\n",
       "      <td>155.27</td>\n",
       "      <td>150.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20200807_Wells_Fargo_META_FB-_2.5B_Person_Plat...</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>META</td>\n",
       "      <td>Meta Platforms Inc.</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>[Solid 2Q Beat, E-Commerce Initiatives Remain ...</td>\n",
       "      <td>300.0</td>\n",
       "      <td>overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>253.67</td>\n",
       "      <td>303.91</td>\n",
       "      <td>248.15</td>\n",
       "      <td>294.68</td>\n",
       "      <td>245.64</td>\n",
       "      <td>329.51</td>\n",
       "      <td>254.69</td>\n",
       "      <td>373.28</td>\n",
       "      <td>302.55</td>\n",
       "      <td>300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20200918_Barclays_GM_General_Motors-_Time_to_t...</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>GM</td>\n",
       "      <td>General Motors Co.</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>[CEO Mary Barra earlier today presented at a c...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>31.58</td>\n",
       "      <td>46.46</td>\n",
       "      <td>28.74</td>\n",
       "      <td>59.26</td>\n",
       "      <td>40.51</td>\n",
       "      <td>63.92</td>\n",
       "      <td>53.76</td>\n",
       "      <td>61.76</td>\n",
       "      <td>48.18</td>\n",
       "      <td>39.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20200722_Barclays_BKNG_Booking_Holdings_Inc.-_...</td>\n",
       "      <td>2020-07-16</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>BKNG</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[The Key Takeaway: BKNGs 2Q results shouldnt b...</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>1732.19</td>\n",
       "      <td>1948.73</td>\n",
       "      <td>1638.47</td>\n",
       "      <td>2281.54</td>\n",
       "      <td>1604.13</td>\n",
       "      <td>2476.90</td>\n",
       "      <td>1886.09</td>\n",
       "      <td>2505.10</td>\n",
       "      <td>2144.72</td>\n",
       "      <td>1950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20211104_Deutsche_Bank_X_US_Steel-_3Q21_EBITDA...</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>X</td>\n",
       "      <td>United States Steel</td>\n",
       "      <td>Materials</td>\n",
       "      <td>[3Q21 FCF of $1.3bn, ND down 46% QoQ, boosts S...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>buy</td>\n",
       "      <td>...</td>\n",
       "      <td>26.39</td>\n",
       "      <td>26.88</td>\n",
       "      <td>18.59</td>\n",
       "      <td>38.45</td>\n",
       "      <td>19.54</td>\n",
       "      <td>32.23</td>\n",
       "      <td>17.02</td>\n",
       "      <td>25.84</td>\n",
       "      <td>18.12</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>913</td>\n",
       "      <td>20150522_Gilford_Securities_Inc_FL_Report_rece...</td>\n",
       "      <td>2015-05-22</td>\n",
       "      <td>Gilford Securities Inc</td>\n",
       "      <td>FL</td>\n",
       "      <td>Foot Locker</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>[Investment Opinion: Foot Locker continues to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buy</td>\n",
       "      <td>...</td>\n",
       "      <td>63.46</td>\n",
       "      <td>74.12</td>\n",
       "      <td>62.00</td>\n",
       "      <td>75.76</td>\n",
       "      <td>58.04</td>\n",
       "      <td>69.14</td>\n",
       "      <td>60.65</td>\n",
       "      <td>67.25</td>\n",
       "      <td>54.77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>914</td>\n",
       "      <td>20200526_Wells_Fargo_HD_HD-_Q2_Rollover_Falls_...</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>HD</td>\n",
       "      <td>Home Depot Inc.</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>[With Q1 strength continuing into May, seeing ...</td>\n",
       "      <td>270.0</td>\n",
       "      <td>overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>238.19</td>\n",
       "      <td>288.24</td>\n",
       "      <td>238.10</td>\n",
       "      <td>291.93</td>\n",
       "      <td>265.70</td>\n",
       "      <td>285.08</td>\n",
       "      <td>261.72</td>\n",
       "      <td>341.12</td>\n",
       "      <td>250.93</td>\n",
       "      <td>270.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>915</td>\n",
       "      <td>20220125_Deutsche_Bank_WFC_Wells_Fargo-_More_C...</td>\n",
       "      <td>2022-01-17</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>WFC</td>\n",
       "      <td>Wells Fargo &amp; Co.</td>\n",
       "      <td>Financials</td>\n",
       "      <td>[WFC posted a solid and mostly in-line 4Q deta...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>buy</td>\n",
       "      <td>...</td>\n",
       "      <td>56.69</td>\n",
       "      <td>59.06</td>\n",
       "      <td>45.81</td>\n",
       "      <td>48.65</td>\n",
       "      <td>37.43</td>\n",
       "      <td>46.14</td>\n",
       "      <td>40.01</td>\n",
       "      <td>47.95</td>\n",
       "      <td>40.68</td>\n",
       "      <td>55.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>916</td>\n",
       "      <td>20200212_Barclays_GM_General_Motors-_Waiting_p...</td>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>GM</td>\n",
       "      <td>General Motors Co.</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>[NEUTRAL Industry View Solid execution and ong...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>33.63</td>\n",
       "      <td>35.49</td>\n",
       "      <td>16.80</td>\n",
       "      <td>30.68</td>\n",
       "      <td>21.46</td>\n",
       "      <td>37.47</td>\n",
       "      <td>26.62</td>\n",
       "      <td>55.86</td>\n",
       "      <td>37.47</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>917</td>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>[Ahead of 4Q13 earnings we are updating our Go...</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>hold</td>\n",
       "      <td>...</td>\n",
       "      <td>29.15</td>\n",
       "      <td>30.53</td>\n",
       "      <td>26.89</td>\n",
       "      <td>30.26</td>\n",
       "      <td>25.90</td>\n",
       "      <td>30.27</td>\n",
       "      <td>26.15</td>\n",
       "      <td>28.39</td>\n",
       "      <td>24.85</td>\n",
       "      <td>26.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>917 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id                                           filename  \\\n",
       "0              1  20161018_Needham_META_Facebook-_3Q16_Preview_R...   \n",
       "1              2  20200807_Wells_Fargo_META_FB-_2.5B_Person_Plat...   \n",
       "2              3  20200918_Barclays_GM_General_Motors-_Time_to_t...   \n",
       "3              4  20200722_Barclays_BKNG_Booking_Holdings_Inc.-_...   \n",
       "4              5  20211104_Deutsche_Bank_X_US_Steel-_3Q21_EBITDA...   \n",
       "..           ...                                                ...   \n",
       "912          913  20150522_Gilford_Securities_Inc_FL_Report_rece...   \n",
       "913          914  20200526_Wells_Fargo_HD_HD-_Q2_Rollover_Falls_...   \n",
       "914          915  20220125_Deutsche_Bank_WFC_Wells_Fargo-_More_C...   \n",
       "915          916  20200212_Barclays_GM_General_Motors-_Waiting_p...   \n",
       "916          917  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...   \n",
       "\n",
       "           date                provider ticker          company_name  \\\n",
       "0    2016-10-11                 Needham   META   Meta Platforms Inc.   \n",
       "1    2020-07-31             Wells Fargo   META   Meta Platforms Inc.   \n",
       "2    2020-09-14                Barclays     GM    General Motors Co.   \n",
       "3    2020-07-16                Barclays   BKNG  Booking Holdings Inc   \n",
       "4    2021-10-28           Deutsche Bank      X   United States Steel   \n",
       "..          ...                     ...    ...                   ...   \n",
       "912  2015-05-22  Gilford Securities Inc     FL           Foot Locker   \n",
       "913  2020-05-19             Wells Fargo     HD       Home Depot Inc.   \n",
       "914  2022-01-17           Deutsche Bank    WFC     Wells Fargo & Co.   \n",
       "915  2020-02-06                Barclays     GM    General Motors Co.   \n",
       "916  2014-01-21  Pivotal Research Group  GOOGL         Alphabet Inc.   \n",
       "\n",
       "                   industry  \\\n",
       "0    Communication Services   \n",
       "1    Communication Services   \n",
       "2                Automobile   \n",
       "3                    Travel   \n",
       "4                 Materials   \n",
       "..                      ...   \n",
       "912                Clothing   \n",
       "913  Consumer Discretionary   \n",
       "914              Financials   \n",
       "915              Automobile   \n",
       "916  Communication Services   \n",
       "\n",
       "                                            paragraphs  target_price  \\\n",
       "0    [INVESTMENT HIGHLIGHTS: $150.00 We raise our e...         150.0   \n",
       "1    [Solid 2Q Beat, E-Commerce Initiatives Remain ...         300.0   \n",
       "2    [CEO Mary Barra earlier today presented at a c...          39.0   \n",
       "3    [The Key Takeaway: BKNGs 2Q results shouldnt b...        1950.0   \n",
       "4    [3Q21 FCF of $1.3bn, ND down 46% QoQ, boosts S...          50.0   \n",
       "..                                                 ...           ...   \n",
       "912  [Investment Opinion: Foot Locker continues to ...           NaN   \n",
       "913  [With Q1 strength continuing into May, seeing ...         270.0   \n",
       "914  [WFC posted a solid and mostly in-line 4Q deta...          55.0   \n",
       "915  [NEUTRAL Industry View Solid execution and ong...          44.0   \n",
       "916  [Ahead of 4Q13 earnings we are updating our Go...        1070.0   \n",
       "\n",
       "         rating  ...  one day after  max price after 3 months  \\\n",
       "0           buy  ...         129.05                    133.28   \n",
       "1    overweight  ...         253.67                    303.91   \n",
       "2    overweight  ...          31.58                     46.46   \n",
       "3    overweight  ...        1732.19                   1948.73   \n",
       "4           buy  ...          26.39                     26.88   \n",
       "..          ...  ...            ...                       ...   \n",
       "912         buy  ...          63.46                     74.12   \n",
       "913  overweight  ...         238.19                    288.24   \n",
       "914         buy  ...          56.69                     59.06   \n",
       "915  overweight  ...          33.63                     35.49   \n",
       "916        hold  ...          29.15                     30.53   \n",
       "\n",
       "     min price after 3 months  max price after 6 months  \\\n",
       "0                      115.05                    142.65   \n",
       "1                      248.15                    294.68   \n",
       "2                       28.74                     59.26   \n",
       "3                     1638.47                   2281.54   \n",
       "4                       18.59                     38.45   \n",
       "..                        ...                       ...   \n",
       "912                     62.00                     75.76   \n",
       "913                    238.10                    291.93   \n",
       "914                     45.81                     48.65   \n",
       "915                     16.80                     30.68   \n",
       "916                     26.89                     30.26   \n",
       "\n",
       "     min price after 6 months  max price after 9 months  \\\n",
       "0                      126.09                    155.27   \n",
       "1                      245.64                    329.51   \n",
       "2                       40.51                     63.92   \n",
       "3                     1604.13                   2476.90   \n",
       "4                       19.54                     32.23   \n",
       "..                        ...                       ...   \n",
       "912                     58.04                     69.14   \n",
       "913                    265.70                    285.08   \n",
       "914                     37.43                     46.14   \n",
       "915                     21.46                     37.47   \n",
       "916                     25.90                     30.27   \n",
       "\n",
       "     min price after 9 months  max price after 12 months  \\\n",
       "0                      139.39                     173.51   \n",
       "1                      254.69                     373.28   \n",
       "2                       53.76                      61.76   \n",
       "3                     1886.09                    2505.10   \n",
       "4                       17.02                      25.84   \n",
       "..                        ...                        ...   \n",
       "912                     60.65                      67.25   \n",
       "913                    261.72                     341.12   \n",
       "914                     40.01                      47.95   \n",
       "915                     26.62                      55.86   \n",
       "916                     26.15                      28.39   \n",
       "\n",
       "     min price after 12 months  adjusted_target_price  \n",
       "0                       155.27                 150.00  \n",
       "1                       302.55                 300.00  \n",
       "2                        48.18                  39.00  \n",
       "3                      2144.72                1950.00  \n",
       "4                        18.12                  50.00  \n",
       "..                         ...                    ...  \n",
       "912                      54.77                    NaN  \n",
       "913                     250.93                 270.00  \n",
       "914                      40.68                  55.00  \n",
       "915                      37.47                  44.00  \n",
       "916                      24.85                  26.65  \n",
       "\n",
       "[917 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# based on Bloomberg data\n",
    "stock_splits = {\n",
    "    \"AAPL\": [\n",
    "        {\"split_ratio\": 7, \"ex_date\": \"2014-06-09\"},\n",
    "        {\"split_ratio\": 4, \"ex_date\": \"2020-08-31\"}\n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        {\"split_ratio\": 20, \"ex_date\": \"2022-06-06\"}\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        {\"split_ratio\": 5, \"ex_date\": \"2020-08-31\"},\n",
    "        {\"split_ratio\": 3, \"ex_date\": \"2022-08-25\"}\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        {\"split_ratio\": 20, \"ex_date\": \"2022-07-18\"},\n",
    "        {\"split_ratio\": 1.0027455, \"ex_date\": \"2015-04-27\"},\n",
    "        {\"split_ratio\": 2.002, \"ex_date\": \"2014-03-27\"},\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        {\"split_ratio\": 10, \"ex_date\": \"2024-06-10\"},\n",
    "        {\"split_ratio\": 4, \"ex_date\": \"2021-07-20\"}\n",
    "    ],\n",
    "    \"NKE\": [\n",
    "        {\"split_ratio\": 2, \"ex_date\": \"2015-12-24\"}\n",
    "    ],\n",
    "    \"V\": [\n",
    "        {\"split_ratio\": 4, \"ex_date\": \"2015-03-19\"}\n",
    "    ],\n",
    "    \"MA\": [\n",
    "        {\"split_ratio\": 10, \"ex_date\": \"2014-01-22\"}\n",
    "    ],\n",
    "    \"WMT\": [\n",
    "        {\"split_ratio\": 3, \"ex_date\": \"2024-02-26\"}\n",
    "    ],\n",
    "    \"SBUX\": [\n",
    "        {\"split_ratio\": 2, \"ex_date\": \"2015-04-09\"}\n",
    "    ],\n",
    "    \"UNP\": [\n",
    "        {\"split_ratio\": 2, \"ex_date\": \"2014-06-09\"}\n",
    "    ],\n",
    "    \"UAA\": [\n",
    "        {\"split_ratio\": 2, \"ex_date\": \"2014-04-15\"}\n",
    "    ],\n",
    "    \"NBR\": [\n",
    "        {\"split_ratio\": 0.02, \"ex_date\": \"2020-04-23\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "def adjust_target_price(target_price, report_date, ticker, stock_splits):\n",
    "    if pd.isna(target_price):\n",
    "        return None  # Handle NaN target prices\n",
    "      \n",
    "    splits = stock_splits.get(ticker, [])\n",
    "    \n",
    "    # update target price based on timeframe of the split\n",
    "    for split in splits:\n",
    "        split_date = datetime.strptime(split[\"ex_date\"], \"%Y-%m-%d\").date()\n",
    "        if split_date > report_date:\n",
    "            target_price /= split[\"split_ratio\"]\n",
    "    \n",
    "    return round(target_price,2)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df_saved_reports[\"adjusted_target_price\"] = df_saved_reports.apply(\n",
    "    lambda row: adjust_target_price(\n",
    "        target_price=row[\"target_price\"],\n",
    "        report_date=row[\"date\"],\n",
    "        ticker=row[\"ticker\"],\n",
    "        stock_splits=stock_splits\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_saved_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate performance of Target Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the adjusted_target_price is below the current price, we need to check if it was below the adjusted_target_price, basically handling buy/sell/hold recommendations\n",
    "def classifcy_performance(df):\n",
    "    if df[\"adjusted_target_price\"] > df[\"start price\"]:\n",
    "        df[\"tp reached after 3 months\"] = df[\"adjusted_target_price\"] <= df[\"max price after 3 months\"]\n",
    "        df[\"tp reached after 6 months\"] = df[\"adjusted_target_price\"] <= df[\"max price after 6 months\"]\n",
    "        df[\"tp reached after 9 months\"] = df[\"adjusted_target_price\"] <= df[\"max price after 9 months\"]\n",
    "        df[\"tp reached after 12 months\"] = df[\"adjusted_target_price\"] <= df[\"max price after 12 months\"]\n",
    "    else:\n",
    "        df[\"tp reached after 3 months\"] = df[\"adjusted_target_price\"] >= df[\"min price after 3 months\"]\n",
    "        df[\"tp reached after 6 months\"] = df[\"adjusted_target_price\"] >= df[\"min price after 6 months\"]\n",
    "        df[\"tp reached after 9 months\"] = df[\"adjusted_target_price\"] >= df[\"min price after 9 months\"]\n",
    "        df[\"tp reached after 12 months\"] = df[\"adjusted_target_price\"] >= df[\"min price after 12 months\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "df_saved_reports = df_saved_reports.apply(classifcy_performance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports.to_csv(\"reports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean/Filter Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports.to_csv(\"/Users/oskarroeske/Masterthesis/argument_extraction/preprocessed_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 917 entries, 0 to 916\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   document_id                 917 non-null    int64  \n",
      " 1   filename                    917 non-null    object \n",
      " 2   date                        917 non-null    object \n",
      " 3   provider                    917 non-null    object \n",
      " 4   ticker                      917 non-null    object \n",
      " 5   company_name                917 non-null    object \n",
      " 6   industry                    917 non-null    object \n",
      " 7   paragraphs                  917 non-null    object \n",
      " 8   target_price                706 non-null    float64\n",
      " 9   rating                      917 non-null    object \n",
      " 10  start price                 917 non-null    float64\n",
      " 11  one day after               917 non-null    float64\n",
      " 12  max price after 3 months    917 non-null    float64\n",
      " 13  min price after 3 months    917 non-null    float64\n",
      " 14  max price after 6 months    917 non-null    float64\n",
      " 15  min price after 6 months    917 non-null    float64\n",
      " 16  max price after 9 months    917 non-null    float64\n",
      " 17  min price after 9 months    917 non-null    float64\n",
      " 18  max price after 12 months   917 non-null    float64\n",
      " 19  min price after 12 months   917 non-null    float64\n",
      " 20  adjusted_target_price       706 non-null    float64\n",
      " 21  tp reached after 3 months   917 non-null    bool   \n",
      " 22  tp reached after 6 months   917 non-null    bool   \n",
      " 23  tp reached after 9 months   917 non-null    bool   \n",
      " 24  tp reached after 12 months  917 non-null    bool   \n",
      "dtypes: bool(4), float64(12), int64(1), object(8)\n",
      "memory usage: 154.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_saved_reports.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the cleaned list into separate rows\n",
    "df_exploded = df_saved_reports.explode('paragraphs').reset_index(drop=True)\n",
    "df_exploded = df_exploded[[\"filename\",\"document_id\",\"provider\",\"ticker\",\"date\",\"industry\",\"paragraphs\"]]\n",
    "df_exploded['paragraph_id'] = df_exploded.groupby('document_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document_id</th>\n",
       "      <th>provider</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>industry</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraph_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>INVESTMENT HIGHLIGHTS: $150.00 We raise our es...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>We expect mobile ad revenue to represent appro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>We raise our estimates for FY16 as we expect a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>We raise our estimates for FY17 and now expect...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Needham</td>\n",
       "      <td>META</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>We are buyers of FB based on our belief that d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Investors will need to consider the following ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Much of online advertising is highly competiti...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>SMEs have been the core . segment of marketers...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>A looming threat for all web publishers relate...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>Pivotal Research Group</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Google has become so . large and so dominant i...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  document_id  \\\n",
       "0      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "1      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "2      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "3      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "4      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "...                                                  ...          ...   \n",
       "13235  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "13236  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "13237  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "13238  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "13239  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "\n",
       "                     provider ticker        date                industry  \\\n",
       "0                     Needham   META  2016-10-11  Communication Services   \n",
       "1                     Needham   META  2016-10-11  Communication Services   \n",
       "2                     Needham   META  2016-10-11  Communication Services   \n",
       "3                     Needham   META  2016-10-11  Communication Services   \n",
       "4                     Needham   META  2016-10-11  Communication Services   \n",
       "...                       ...    ...         ...                     ...   \n",
       "13235  Pivotal Research Group  GOOGL  2014-01-21  Communication Services   \n",
       "13236  Pivotal Research Group  GOOGL  2014-01-21  Communication Services   \n",
       "13237  Pivotal Research Group  GOOGL  2014-01-21  Communication Services   \n",
       "13238  Pivotal Research Group  GOOGL  2014-01-21  Communication Services   \n",
       "13239  Pivotal Research Group  GOOGL  2014-01-21  Communication Services   \n",
       "\n",
       "                                              paragraphs  paragraph_id  \n",
       "0      INVESTMENT HIGHLIGHTS: $150.00 We raise our es...             1  \n",
       "1      We expect mobile ad revenue to represent appro...             2  \n",
       "2      We raise our estimates for FY16 as we expect a...             3  \n",
       "3      We raise our estimates for FY17 and now expect...             4  \n",
       "4      We are buyers of FB based on our belief that d...             5  \n",
       "...                                                  ...           ...  \n",
       "13235  Investors will need to consider the following ...            25  \n",
       "13236  Much of online advertising is highly competiti...            26  \n",
       "13237  SMEs have been the core . segment of marketers...            27  \n",
       "13238  A looming threat for all web publishers relate...            28  \n",
       "13239  Google has become so . large and so dominant i...            29  \n",
       "\n",
       "[13240 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "# Load spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a customized sentence boundary detection component to spaCy\n",
    "@Language.component(\"custom_sentencizer\")\n",
    "def custom_sentencizer(doc):\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        # Always split at '!' or '?'\n",
    "        if token.text in [\"!\", \"?\"]:\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "\n",
    "        # Split at '.' only if not part of a number or abbreviation\n",
    "        if token.text == \".\":\n",
    "            next_token = token.nbor(1) if token.i + 1 < len(doc) else None\n",
    "            prev_token = token.nbor(-1) if token.i - 1 >= 0 else None\n",
    "            prev_prev_token = token.nbor(-2) if token.i - 2 >= 0 else None\n",
    "            \n",
    "            # Check if next token starts a title-case word\n",
    "            if next_token and next_token.is_title:\n",
    "                doc[token.i + 1].is_sent_start = True\n",
    "            \n",
    "            # Prevent splitting for abbreviations\n",
    "            if prev_token and prev_token.text.lower() in {\"mr\", \"ms\", \"dr\", \"etc\", \"e.g\", \"adj\", \"sr\"}:\n",
    "                doc[token.i + 1].is_sent_start = False\n",
    "            \n",
    "            # Prevent splitting within numbers (e.g., 3.14)\n",
    "            if prev_token and prev_token.like_num and next_token and next_token.like_num:\n",
    "                doc[token.i + 1].is_sent_start = False\n",
    "\n",
    "            # Prevent splitting if there are at least two uppercase words before the period\n",
    "            if (\n",
    "                prev_prev_token and prev_prev_token.text.isupper() and\n",
    "                prev_token and prev_token.text.isupper() and\n",
    "                next_token and next_token.is_lower\n",
    "            ):\n",
    "                doc[token.i + 1].is_sent_start = False\n",
    "\n",
    "    return doc\n",
    "# Add the custom sentencizer to the pipeline\n",
    "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")\n",
    "\n",
    "# Function to split paragraphs into sentences\n",
    "def split_into_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []  # Return an empty list for non-string values\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "result = []\n",
    "\n",
    "def check_sentence_validity(sentence):\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "    has_verb = False\n",
    "    has_subject = False\n",
    "\n",
    "    sentence = nlp(sentence)\n",
    "        \n",
    "    for token in sentence:\n",
    "        # Check for a verb\n",
    "        if token.pos_ in {\"VERB\", \"AUX\"}:\n",
    "            has_verb = True\n",
    "        # Check for a subject\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "            has_subject = True\n",
    " \n",
    "    # If both a verb and a subject are found, the sentence is valid\n",
    "    if has_verb and has_subject:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Split each paragraph into sentences\n",
    "for _, row in df_exploded.iterrows():\n",
    "    filename = row['filename']\n",
    "    doc_id = row['document_id']\n",
    "    paragraph_id = row['paragraph_id']\n",
    "    text = row['paragraphs']\n",
    "    sentences = split_into_sentences(text)\n",
    "    for sentence_id, sentence in enumerate(sentences, start=1):\n",
    "        is_valid_sentence = check_sentence_validity(sentence)\n",
    "        # Check if the sentence has more than 5 words\n",
    "        if len(sentence.split()) >= 5 and is_valid_sentence:\n",
    "            result.append({\n",
    "                'filename':filename,\n",
    "                'document_id': doc_id,\n",
    "                'paragraph_id': paragraph_id,\n",
    "                'sentence_id': sentence_id,\n",
    "                'sentence': sentence\n",
    "            })\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "df_preprocessed_sentences = pd.DataFrame(result)\n",
    "\n",
    "# Filter sentences from the table of contents -> caused issues before\n",
    "df_preprocessed_sentences = df_preprocessed_sentences[~df_preprocessed_sentences['sentence'].str.contains(r\"\\.{5,}\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>INVESTMENT HIGHLIGHTS: $150.00 We raise our es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>We now expect 3Q16 revenue of $6.855B up 52% y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>FB will report 3Q16 earnings on Wednesday, Nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The call in number is , ID .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20161018_Needham_META_Facebook-_3Q16_Preview_R...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Advertising We maintain our Buy rating and $15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36459</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>Changes in that industrys presence or its reli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36460</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>A looming threat for all web publishers relate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36461</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>Rules could be established in the future which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36462</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Google has become so .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36463</th>\n",
       "      <td>20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...</td>\n",
       "      <td>917</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>large and so dominant in a number of sectors o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36459 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  document_id  \\\n",
       "0      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "1      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "2      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "3      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "4      20161018_Needham_META_Facebook-_3Q16_Preview_R...            1   \n",
       "...                                                  ...          ...   \n",
       "36459  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "36460  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "36461  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "36462  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "36463  20140122_Pivotal_Research_Group_GOOGL_GOOG-_Gr...          917   \n",
       "\n",
       "       paragraph_id  sentence_id  \\\n",
       "0                 1            1   \n",
       "1                 1            2   \n",
       "2                 1            3   \n",
       "3                 1            4   \n",
       "4                 1            6   \n",
       "...             ...          ...   \n",
       "36459            27            6   \n",
       "36460            28            1   \n",
       "36461            28            2   \n",
       "36462            29            1   \n",
       "36463            29            2   \n",
       "\n",
       "                                                sentence  \n",
       "0      INVESTMENT HIGHLIGHTS: $150.00 We raise our es...  \n",
       "1      We now expect 3Q16 revenue of $6.855B up 52% y...  \n",
       "2      FB will report 3Q16 earnings on Wednesday, Nov...  \n",
       "3                           The call in number is , ID .  \n",
       "4      Advertising We maintain our Buy rating and $15...  \n",
       "...                                                  ...  \n",
       "36459  Changes in that industrys presence or its reli...  \n",
       "36460  A looming threat for all web publishers relate...  \n",
       "36461  Rules could be established in the future which...  \n",
       "36462                             Google has become so .  \n",
       "36463  large and so dominant in a number of sectors o...  \n",
       "\n",
       "[36459 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_sentences.to_csv(\"/Users/oskarroeske/Masterthesis/argument_extraction/preprocessed_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sentences back into paragraphs\n",
    "df_preprocessed_paragraphs = (\n",
    "    df_preprocessed_sentences\n",
    "    .groupby(['filename','document_id','paragraph_id'])  # Group by paragraph_id\n",
    "    .agg({'sentence': ' '.join})  # Concatenate sentences within each group\n",
    "    .reset_index()  # Reset index to keep paragraph_id as a column\n",
    ")\n",
    "\n",
    "# Rename the column to 'paragraph' for clarity\n",
    "df_preprocessed_paragraphs.rename(columns={'sentence': 'paragraph'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>Last night, ahead of the 2014 CES, NVDA hosted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>GameStream works hand in hand with the PC gami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>NVDA sought to bridge the gap between PC and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>Recognizing the shift to advanced automobile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>NVDA has spent heavily on its mobile applicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12568</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>Even though available at quite at a lag with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12569</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>2</td>\n",
       "      <td>Coming back to the October data, International...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12570</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>International shipments Apple till October hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12571</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>4</td>\n",
       "      <td>5G mix accounted for 80% of total shipments in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12572</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>5</td>\n",
       "      <td>tracked at 266.1 mn units, accounting for 76% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12573 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  document_id  \\\n",
       "0      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "1      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "2      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "3      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "4      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "...                                                  ...          ...   \n",
       "12568  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12569  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12570  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12571  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12572  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "\n",
       "       paragraph_id                                          paragraph  \n",
       "0                 1  Last night, ahead of the 2014 CES, NVDA hosted...  \n",
       "1                 2  GameStream works hand in hand with the PC gami...  \n",
       "2                 3  NVDA sought to bridge the gap between PC and m...  \n",
       "3                 4  Recognizing the shift to advanced automobile f...  \n",
       "4                 5  NVDA has spent heavily on its mobile applicati...  \n",
       "...             ...                                                ...  \n",
       "12568             1  Even though available at quite at a lag with t...  \n",
       "12569             2  Coming back to the October data, International...  \n",
       "12570             3  International shipments Apple till October hav...  \n",
       "12571             4  5G mix accounted for 80% of total shipments in...  \n",
       "12572             5  tracked at 266.1 mn units, accounting for 76% ...  \n",
       "\n",
       "[12573 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_counts = df_preprocessed_paragraphs.groupby(\"document_id\").size().reset_index(name=\"paragraph_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id  paragraph_count\n",
       "51            52                1\n",
       "593          595                1\n",
       "690          692                1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_counts[paragraph_counts[\"paragraph_count\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>20170302_BTIG_PRGO_Perrigo_Company_plc.pdf</td>\n",
       "      <td>692</td>\n",
       "      <td>1</td>\n",
       "      <td>Shares of PRGO were down more than 10% after t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  document_id  paragraph_id  \\\n",
       "2655  20170302_BTIG_PRGO_Perrigo_Company_plc.pdf          692             1   \n",
       "\n",
       "                                              paragraph  \n",
       "2655  Shares of PRGO were down more than 10% after t...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_paragraphs[df_preprocessed_paragraphs[\"document_id\"]==692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>provider</th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>target_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>min price after 6 months</th>\n",
       "      <th>max price after 9 months</th>\n",
       "      <th>min price after 9 months</th>\n",
       "      <th>max price after 12 months</th>\n",
       "      <th>min price after 12 months</th>\n",
       "      <th>adjusted_target_price</th>\n",
       "      <th>tp reached after 3 months</th>\n",
       "      <th>tp reached after 6 months</th>\n",
       "      <th>tp reached after 9 months</th>\n",
       "      <th>tp reached after 12 months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>584</td>\n",
       "      <td>20150323_Needham_PRGO_PRGO-_RX_Metrics-_PRGO_S...</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td>Needham</td>\n",
       "      <td>PRGO</td>\n",
       "      <td>Perrigo</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[Enclosed within the following report are week...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hold</td>\n",
       "      <td>...</td>\n",
       "      <td>177.07</td>\n",
       "      <td>182.39</td>\n",
       "      <td>145.59</td>\n",
       "      <td>149.45</td>\n",
       "      <td>124.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id                                           filename  \\\n",
       "583          584  20150323_Needham_PRGO_PRGO-_RX_Metrics-_PRGO_S...   \n",
       "\n",
       "           date provider ticker company_name    industry  \\\n",
       "583  2015-03-16  Needham   PRGO      Perrigo  Healthcare   \n",
       "\n",
       "                                            paragraphs  target_price rating  \\\n",
       "583  [Enclosed within the following report are week...           NaN   hold   \n",
       "\n",
       "     ...  min price after 6 months  max price after 9 months  \\\n",
       "583  ...                    177.07                    182.39   \n",
       "\n",
       "     min price after 9 months  max price after 12 months  \\\n",
       "583                    145.59                     149.45   \n",
       "\n",
       "     min price after 12 months  adjusted_target_price  \\\n",
       "583                     124.08                    NaN   \n",
       "\n",
       "     tp reached after 3 months  tp reached after 6 months  \\\n",
       "583                      False                      False   \n",
       "\n",
       "     tp reached after 9 months  tp reached after 12 months  \n",
       "583                      False                       False  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saved_reports[df_saved_reports[\"document_id\"]==584]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_paragraphs.to_csv(\"/Users/oskarroeske/Masterthesis/argument_extraction/preprocessed_paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>Last night, ahead of the 2014 CES, NVDA hosted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>GameStream works hand in hand with the PC gami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>NVDA sought to bridge the gap between PC and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>Recognizing the shift to advanced automobile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20140114_Needham_NVDA_NVDA-_Compelling_Technol...</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>NVDA has spent heavily on its mobile applicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12568</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>Even though available at quite at a lag with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12569</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>2</td>\n",
       "      <td>Coming back to the October data, International...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12570</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>International shipments Apple till October hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12571</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>4</td>\n",
       "      <td>5G mix accounted for 80% of total shipments in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12572</th>\n",
       "      <td>20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...</td>\n",
       "      <td>203</td>\n",
       "      <td>5</td>\n",
       "      <td>tracked at 266.1 mn units, accounting for 76% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12573 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  document_id  \\\n",
       "0      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "1      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "2      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "3      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "4      20140114_Needham_NVDA_NVDA-_Compelling_Technol...          512   \n",
       "...                                                  ...          ...   \n",
       "12568  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12569  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12570  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12571  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "12572  20221230_JP_Morgan_AAPL_Apple-_Public_CAICT_Ch...          203   \n",
       "\n",
       "       paragraph_id                                          paragraph  \n",
       "0                 1  Last night, ahead of the 2014 CES, NVDA hosted...  \n",
       "1                 2  GameStream works hand in hand with the PC gami...  \n",
       "2                 3  NVDA sought to bridge the gap between PC and m...  \n",
       "3                 4  Recognizing the shift to advanced automobile f...  \n",
       "4                 5  NVDA has spent heavily on its mobile applicati...  \n",
       "...             ...                                                ...  \n",
       "12568             1  Even though available at quite at a lag with t...  \n",
       "12569             2  Coming back to the October data, International...  \n",
       "12570             3  International shipments Apple till October hav...  \n",
       "12571             4  5G mix accounted for 80% of total shipments in...  \n",
       "12572             5  tracked at 266.1 mn units, accounting for 76% ...  \n",
       "\n",
       "[12573 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_paragraphs['token_count'] = df_preprocessed_paragraphs['paragraph'].apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tokens in the column: 1009\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum token count\n",
    "max_tokens = df_preprocessed_paragraphs['token_count'].max()\n",
    "print(f\"Maximum tokens in the column: {max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles for token counts:\n",
      "0.25      37.00\n",
      "0.50      73.00\n",
      "0.75     127.00\n",
      "0.90     194.00\n",
      "0.95     249.00\n",
      "0.97     298.00\n",
      "0.98     338.56\n",
      "0.99     420.28\n",
      "1.00    1009.00\n",
      "Name: token_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate df_preprocessed_paragraphs\n",
    "percentiles = df_preprocessed_paragraphs['token_count'].quantile([0.25, 0.5, 0.75, 0.9,0.95,0.97,0.98,0.99,1.0])\n",
    "\n",
    "# Display percentiles\n",
    "print(\"Percentiles for token counts:\")\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
