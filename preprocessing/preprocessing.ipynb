{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.5.4)\n",
      "Requirement already satisfied: tabula in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.10.19)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (75.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy tabula https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the provider info CSV for ending keywords\n",
    "provider_info = pd.read_csv('provider.csv')\n",
    "\n",
    "# Load the company info CSV for ticker validation and company metadata\n",
    "company_info = pd.read_csv('company_info.csv')  # Replace with the actual path\n",
    "\n",
    "company_info = company_info.drop_duplicates(subset='Ticker Symbol')\n",
    "\n",
    "# Create a dictionary to map ticker symbols to company name and industry\n",
    "ticker_map = company_info.set_index('Ticker Symbol')[['Company Name', 'Industry']].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Only relevant to check font type and size for\\nimport pdfplumber\\n\\ndef extract_words_with_formatting(page):\\n\\n    # Extract words with their bounding boxes\\n    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\\n\\n    formatted_words = []\\n    for word in words:\\n        formatted_words.append({\\n            \"word\": word[\"text\"],\\n            \"font\": word.get(\"fontname\", \"Unknown\"),\\n            \"size\": word.get(\"size\", \"Unknown\"),\\n            \"x0\": word[\"x0\"],\\n            \"x1\": word[\"x1\"],\\n            \"top\": word[\"top\"],\\n            \"bottom\": word[\"bottom\"]\\n        })\\n    return formatted_words\\n\\n\\n# Example usage with pdfplumber\\npdf_path = \"/Users/oskarroeske/Masterthesis/preprocessing/testing/20200605_Needham_CRM_COVID_Hits_CRM_Sales-_Guidance_on_Wrong_Side_of_Growth_-.pdf\"\\n\\nwith pdfplumber.open(pdf_path) as pdf:\\n    for page_number, page in enumerate(pdf.pages, start=1):\\n        print(f\"Page {page_number}:\")\\n        formatted_words = extract_words_with_formatting(page)\\n        for word_info in formatted_words:\\n            print(\\n                f\"Word: \\'{word_info[\\'word\\']}\\', Font: {word_info[\\'font\\']}, Size: {word_info[\\'size\\']}, \"\\n                f\"Position: ({word_info[\\'x0\\']}, {word_info[\\'top\\']} - {word_info[\\'x1\\']}, {word_info[\\'bottom\\']})\"\\n            )\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Only relevant to check font type and size for\n",
    "import pdfplumber\n",
    "\n",
    "def extract_words_with_formatting(page):\n",
    "\n",
    "    # Extract words with their bounding boxes\n",
    "    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\n",
    "\n",
    "    formatted_words = []\n",
    "    for word in words:\n",
    "        formatted_words.append({\n",
    "            \"word\": word[\"text\"],\n",
    "            \"font\": word.get(\"fontname\", \"Unknown\"),\n",
    "            \"size\": word.get(\"size\", \"Unknown\"),\n",
    "            \"x0\": word[\"x0\"],\n",
    "            \"x1\": word[\"x1\"],\n",
    "            \"top\": word[\"top\"],\n",
    "            \"bottom\": word[\"bottom\"]\n",
    "        })\n",
    "    return formatted_words\n",
    "\n",
    "\n",
    "# Example usage with pdfplumber\n",
    "pdf_path = \"/Users/oskarroeske/Masterthesis/preprocessing/testing/20200605_Needham_CRM_COVID_Hits_CRM_Sales-_Guidance_on_Wrong_Side_of_Growth_-.pdf\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        print(f\"Page {page_number}:\")\n",
    "        formatted_words = extract_words_with_formatting(page)\n",
    "        for word_info in formatted_words:\n",
    "            print(\n",
    "                f\"Word: '{word_info['word']}', Font: {word_info['font']}, Size: {word_info['size']}, \"\n",
    "                f\"Position: ({word_info['x0']}, {word_info['top']} - {word_info['x1']}, {word_info['bottom']})\"\n",
    "            )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new Structure\n",
    "\n",
    "patterns = {\n",
    "    \"BGC Partners\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\(\\$\\) (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+) \\(\\w+,\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures Appendix\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Needham\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target: \\$(\\d{1,3}(,\\d{3})*(\\.\\d{2})?)\",\n",
    "            \"secondary\": r\"PRICE TARGET: \\$(\\d{1,3}(,\\d{3})*(\\.\\d{2})?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "            \"secondary\": r\"Rating (\\w+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"\n",
    "                            r\"ANALYST CERTIFICATION\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Cambria(-Bold)?(-Regular)?\", \"font_size\": 10.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]KeplerStd(-Bold)?\", \"font_size\": 9.0},\n",
    "        ]\n",
    "    },\n",
    "    \"BTIG\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\$(\\d+(\\.\\d+)?) 12 month target \",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix: Analyst Certification and Other Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Corbel(,Bold)?(,-Italic)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Wells Fargo\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\/Price Target: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price Target\\/Prior: \\$(\\d+(\\.\\d+)?)\",\n",
    "            \"tertiary\": r\"\\/\\$(\\d+(\\.\\d+)?)\"\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+)/\\$\",\n",
    "            \"secondary\": r\"Rating (\\w+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Required Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]WellsFargoSans(-Light)?(,-SemiBold)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Verdana(-Bold)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 9.01}\n",
    "        ]\n",
    "    },\n",
    "    \"Barclays\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target USD (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price Target: USD (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Stock Rating ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"ANALYST\\(S\\) CERTIFICATION\\(S\\)\",\n",
    "                            r\"Analyst\\(s\\) Certification\\(s\\)\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Expert Sans (Extra Bold)?(Regular)?(Regular,Bold)?\", \"font_size\": 9.0},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Expert Sans (Extra Bold)?(Regular)?\", \"font_size\": 8.04},\n",
    "            {\"font_type\": r\"[A-Z]+[+]DejaVuSans(-Bold)?\", \"font_size\": 9.01},\n",
    "        ]\n",
    "    },\n",
    "    \"JP Morgan\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\([A-Za-z0-9\\-]+\\): \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]TimesNewRoman(,Bold)?\", \"font_size\": 9.60},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Brean Capital LLC\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"PT: \\$ (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+) PT:\\$\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Hilliard Lyons\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target (NA|\\$(\\d+(\\.\\d+)?))\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"-- NYSE\\s+[–\\-—]+\\s+([A-Za-z]+)\\s+[–\\-—]+\",\n",
    "            \"secondary\":r\"NYSE\\s+[–\\-—]+\\s+([A-Za-z\\- ]+?)(?=\\s*[-–—]\\d)\"\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Analyst Certification\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Verdana(-Bold)?\", \"font_size\": 9.00},\n",
    "            {\"font_type\":r\"TimesNewRomanPS(-BoldMT)?\", \"font_size\": 10.98},\n",
    "            ]\n",
    "    },\n",
    "    \"Alliance Global Partners\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target (NA|\\$(\\d+(\\.\\d+)?))\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Imporant Research Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]ArialMT(-BoldMT)?\", \"font_size\": 8.00},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Mizuho Securities\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rating ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"IMPORTANT DISCLOSURES\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Gilford Securities Inc\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"\\, \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rated: ([A-Za-z]+)\",\n",
    "            \"secondary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": r\"ANALYST CERTIFICATION\",\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"ArialMT(-BoldMT)?\", \"font_size\": 10.02},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Deutsche Bank\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price Target \\(USD\\) (\\d+(\\.\\d+)?)\",\n",
    "            \"secondary\": r\"Price target (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rating ([A-Za-z]+)\",\n",
    "            \"secondary\": r\"\\b(Buy|Hold|Sell|Overweight|Underperform)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix 1\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]UniversDeutscheBank-Regular\", \"font_size\": 9.0},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Pivotal Research Group\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Target Price: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"RATING: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Appendix: Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Helvetica(-Bold)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Arial\", \"font_size\": 8.04},\n",
    "        ]\n",
    "    },\n",
    "    \"Spartan Capital Securities LLC\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"T \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Important Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Cascend Securities -Historical-\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price target: \\$(\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Rating: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures: \"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(,Bold)?\", \"font_size\": 12.0},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"Phillip Securities\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"TARGET PRICE USD (\\d+(\\.\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"\\b(BUY|HOLD|SELL|OVERWEIGHT|UNDERPERFORM)\\b\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Contact Information\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?\", \"font_size\": 9.96},\n",
    "            {\"font_type\": r\"[A-Z]+[+]Calibri(-Bold)?\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"FinTrust Investment Advisors\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Target Price: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Fintrust Rating: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Important Disclosures:\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Arial(-BoldMT)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    },\n",
    "    \"IBI Investment House\": {\n",
    "        \"price_patterns\": {\n",
    "            \"primary\": r\"Price target: \\$(\\d+(,\\d+)?)\",\n",
    "        },\n",
    "        \"rating_patterns\": {\n",
    "            \"primary\": r\"Recommendation: ([A-Za-z]+)\",\n",
    "        },\n",
    "        \"ending_patterns\": [r\"Disclosures\"],\n",
    "        \"font_patterns\": [\n",
    "            {\"font_type\": r\"Tahoma(-Bold)?\", \"font_size\": 7.92},\n",
    "            {\"font_type\": \"Not Available\", \"font_size\": 10.0},\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(paragraph):\n",
    "    # Parse the paragraph\n",
    "    paragraph = re.sub(r\"\\s+\", \" \", paragraph).strip()\n",
    "    doc = nlp(paragraph)\n",
    "        \n",
    "    for sent in doc.sents:\n",
    "        has_verb = False\n",
    "        has_subject = False\n",
    "        \n",
    "        for token in sent:\n",
    "            # Check for a verb\n",
    "            if token.pos_ in {\"VERB\", \"AUX\"}:\n",
    "                has_verb = True\n",
    "            # Check for a subject\n",
    "            if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                has_subject = True\n",
    "        \n",
    "        # If both a verb and a subject are found, the sentence is valid\n",
    "        if has_verb and has_subject:\n",
    "            return True\n",
    "        \n",
    "        # At least one word with 5+ letters and all upper case\n",
    "        if re.search(r\"[A-Z]{5,}\", paragraph):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_valid_paragraphs(paragraphs):\n",
    "    valid_paragraphs = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if check_validity(paragraph):  # Validate each paragraph\n",
    "            valid_paragraphs.append(paragraph)\n",
    "    return valid_paragraphs\n",
    "\n",
    "def extract_text_with_format(page, provider):\n",
    "    #print(f\"Provider: {provider}\")\n",
    "\n",
    "    provider_patterns = patterns[provider]\n",
    "\n",
    "    # Access patterns\n",
    "    price_patterns = provider_patterns[\"price_patterns\"]\n",
    "    rating_patterns = provider_patterns[\"rating_patterns\"]\n",
    "    ending_patterns = provider_patterns[\"ending_patterns\"]\n",
    "    font_patterns = provider_patterns[\"font_patterns\"]\n",
    "\n",
    "    # Extract words with font and size details\n",
    "    words = page.extract_words(extra_attrs=[\"fontname\", \"size\"])\n",
    "\n",
    "    # Round text sizes to 3 decimal places, moved it to lower part, hope this is correct\n",
    "    \"\"\"for word in words:\n",
    "        if \"size\" in word and word[\"size\"] is not None:\n",
    "            word[\"size\"] = round(word[\"size\"], 2)\"\"\"\n",
    "\n",
    "    # Sort words by vertical and horizontal position\n",
    "    words.sort(key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "    current_top = None\n",
    "\n",
    "    rating = None\n",
    "    price = None\n",
    "\n",
    "    # Lookahead buffer for multi-word patterns\n",
    "    lookahead_buffer = []\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        if \"size\" in word and word[\"size\"] is not None:\n",
    "            word[\"size\"] = round(word[\"size\"], 2)\n",
    "\n",
    "        # Build lookahead buffer\n",
    "        lookahead_buffer.append(word[\"text\"])\n",
    "        if len(lookahead_buffer) > 10:\n",
    "            lookahead_buffer.pop(0)\n",
    "        buffer_text = \" \".join(lookahead_buffer)\n",
    "\n",
    "        #print(buffer_text)\n",
    "\n",
    "        # Extract rating\n",
    "        if not rating:\n",
    "            for pattern_key in [\"primary\", \"secondary\",\"tertiary\"]:\n",
    "                pattern = rating_patterns.get(pattern_key)\n",
    "                if pattern:\n",
    "                    rating_match = re.search(pattern, buffer_text)\n",
    "                    if rating_match:\n",
    "                        rating = rating_match.group(1)\n",
    "                        break\n",
    "\n",
    "        # Extract price\n",
    "        if not price:\n",
    "            for pattern_key in [\"primary\", \"secondary\"]:\n",
    "                pattern = price_patterns.get(pattern_key)\n",
    "                if pattern:\n",
    "                    price_match = re.search(pattern, buffer_text)\n",
    "                    if price_match:\n",
    "                        price = price_match.group(1)\n",
    "                        break\n",
    "\n",
    "        # Check for ending pattern\n",
    "        # Check if buffer matches any ending pattern\n",
    "        for ending_pattern in ending_patterns:\n",
    "            if re.search(ending_pattern, buffer_text):\n",
    "                return filter_valid_paragraphs(paragraphs), True, rating, price\n",
    "\n",
    "        # Match word against font patterns\n",
    "        is_font_matched = False\n",
    "        for font_pattern in font_patterns:\n",
    "            font_type = font_pattern[\"font_type\"]\n",
    "            font_size = font_pattern[\"font_size\"]\n",
    "            if re.match(font_type, word[\"fontname\"]) and word[\"size\"] == font_size:\n",
    "                is_font_matched = True\n",
    "                #break\n",
    "\n",
    "        if not is_font_matched:\n",
    "            continue\n",
    "\n",
    "        # Group words into paragraphs\n",
    "        if current_top is None or abs(word[\"top\"] - current_top) < 13:  # Adjust threshold as needed\n",
    "            current_paragraph.append(word[\"text\"])\n",
    "        else:\n",
    "            # New paragraph starts\n",
    "            paragraphs.append(\" \".join(current_paragraph))\n",
    "            current_paragraph = [word[\"text\"]]\n",
    "\n",
    "        # Update the current top position\n",
    "        current_top = word[\"top\"]\n",
    "\n",
    "    # Add the last paragraph\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    return filter_valid_paragraphs(paragraphs), False, rating, price\n",
    "\n",
    "\n",
    "# NEW VERSION (16.11.2024)\n",
    "def extract_metadata(filename, ticker_map):\n",
    "    \"\"\"\n",
    "    Extract metadata (date, provider, ticker) from the filename using ticker_map.\n",
    "    \"\"\"\n",
    "    # Extract the date (first 8 digits in the filename)\n",
    "    date_match = re.match(r\"(\\d{8})\", filename)\n",
    "    if not date_match:\n",
    "        return None, None, None, None, None\n",
    "    date_str = date_match.group(1)\n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "\n",
    "    #print(f\"fileName: {filename}\")\n",
    "    #print(f\"date: {date}\")\n",
    "\n",
    "    # Look for the ticker in the filename\n",
    "    for ticker in ticker_map.keys():\n",
    "        ticker_pattern = f\"_{ticker}_\"  # Ensure ticker is surrounded by underscores\n",
    "        if ticker_pattern in filename:\n",
    "            # Extract the portion between date and ticker as the provider\n",
    "            provider_section = filename.split(f\"{date_str}_\")[1].split(f\"_{ticker}_\")[0]\n",
    "            provider = provider_section.replace('_', ' ')  # Replace underscores with spaces\n",
    "            # Get company name and industry from the ticker_map\n",
    "            company_name = ticker_map[ticker]['Company Name']\n",
    "            industry = ticker_map[ticker]['Industry']\n",
    "            return date, provider, ticker, company_name, industry\n",
    "\n",
    "    # If no ticker is found, return None for ticker-related fields\n",
    "    return date, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>provider</th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>target_price</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20150305_Needham_CRM_CRM-_Not_Even_FX_Could_Ta...</td>\n",
       "      <td>2015-03-05</td>\n",
       "      <td>Needham</td>\n",
       "      <td>CRM</td>\n",
       "      <td>Salesforce.com Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>[BUY INVESTMENT HIGHLIGHTS:, Q4 was solid, hig...</td>\n",
       "      <td>80.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20171002_JP_Morgan_NEM_Denver_Gold_Show-_Nevad...</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>JP Morgan</td>\n",
       "      <td>NEM</td>\n",
       "      <td>Newmont Corp.</td>\n",
       "      <td>Materials</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20160510_Phillip_Securities_AAPL_Apple_Inc._Th...</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>Phillip Securities</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>[Apple Inc. Snapshot Apple Inc. (AAPL) recentl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20170322_BTIG_NKTR_Nektar_Therapeutics.pdf</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>BTIG</td>\n",
       "      <td>NKTR</td>\n",
       "      <td>Nektar Therapeutics</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[Nektar announced positive results from the Ph...</td>\n",
       "      <td>22.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20180504_Needham_AMZN_Momentum_Remains_High_wi...</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>Needham</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc.</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>[BUY INVESTMENT HIGHLIGHTS:, Amazon posted str...</td>\n",
       "      <td>1,900.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>20181114_BTIG_PRGO_Perrigo_Company_plc.pdf</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>BTIG</td>\n",
       "      <td>PRGO</td>\n",
       "      <td>Perrigo</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[(212) 527-3505 We have lowered our CY18 reven...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20150819_BTIG_PRGO_Perrigo_Company_plc.pdf</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>BTIG</td>\n",
       "      <td>PRGO</td>\n",
       "      <td>Perrigo</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[]</td>\n",
       "      <td>223.00</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>20151223_Needham_TSLA_SCTY-_SCTY_Analyst_Day_H...</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>Needham</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla Inc.</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>[INVESTMENT HIGHLIGHTS: NA SCTY hosted a well-...</td>\n",
       "      <td>None</td>\n",
       "      <td>hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>20150615_Gilford_Securities_Inc_WMT_Report_rec...</td>\n",
       "      <td>2015-06-15</td>\n",
       "      <td>Gilford Securities Inc</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart Inc.</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                           filename       date  \\\n",
       "0            1  20150305_Needham_CRM_CRM-_Not_Even_FX_Could_Ta... 2015-03-05   \n",
       "1            2  20171002_JP_Morgan_NEM_Denver_Gold_Show-_Nevad... 2017-10-02   \n",
       "2            3  20160510_Phillip_Securities_AAPL_Apple_Inc._Th... 2016-05-10   \n",
       "3            4         20170322_BTIG_NKTR_Nektar_Therapeutics.pdf 2017-03-22   \n",
       "4            5  20180504_Needham_AMZN_Momentum_Remains_High_wi... 2018-05-04   \n",
       "5            6         20181114_BTIG_PRGO_Perrigo_Company_plc.pdf 2018-11-14   \n",
       "6            7         20150819_BTIG_PRGO_Perrigo_Company_plc.pdf 2015-08-19   \n",
       "7            8  20151223_Needham_TSLA_SCTY-_SCTY_Analyst_Day_H... 2015-12-23   \n",
       "8            9  20150615_Gilford_Securities_Inc_WMT_Report_rec... 2015-06-15   \n",
       "\n",
       "                 provider ticker         company_name                industry  \\\n",
       "0                 Needham    CRM  Salesforce.com Inc.              Technology   \n",
       "1               JP Morgan    NEM        Newmont Corp.               Materials   \n",
       "2      Phillip Securities   AAPL           Apple Inc.              Technology   \n",
       "3                    BTIG   NKTR  Nektar Therapeutics              Healthcare   \n",
       "4                 Needham   AMZN      Amazon.com Inc.  Consumer Discretionary   \n",
       "5                    BTIG   PRGO              Perrigo              Healthcare   \n",
       "6                    BTIG   PRGO              Perrigo              Healthcare   \n",
       "7                 Needham   TSLA           Tesla Inc.              Automobile   \n",
       "8  Gilford Securities Inc    WMT         Walmart Inc.        Consumer Staples   \n",
       "\n",
       "                                          paragraphs target_price      rating  \n",
       "0  [BUY INVESTMENT HIGHLIGHTS:, Q4 was solid, hig...        80.00         buy  \n",
       "1                                                 []         None  overweight  \n",
       "2  [Apple Inc. Snapshot Apple Inc. (AAPL) recentl...         None        None  \n",
       "3  [Nektar announced positive results from the Ph...        22.00         buy  \n",
       "4  [BUY INVESTMENT HIGHLIGHTS:, Amazon posted str...     1,900.00         buy  \n",
       "5  [(212) 527-3505 We have lowered our CY18 reven...         None        None  \n",
       "6                                                 []       223.00         buy  \n",
       "7  [INVESTMENT HIGHLIGHTS: NA SCTY hosted a well-...         None        hold  \n",
       "8                                                 []         None        None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define function to process all PDFs in the directory and store data in DataFrame\n",
    "def extract_text_from_all_pdfs_to_dataframe(directory_path, provider_info, ticker_map):\n",
    "    data = []\n",
    "    id_counter = 1  # Initialize an ID counter\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            \n",
    "            # Extract metadata from filename\n",
    "            date, provider, ticker, company_name, industry = extract_metadata(filename,ticker_map=ticker_map)\n",
    "\n",
    "            all_paragraphs = []  \n",
    "            first_rating = None\n",
    "            first_price = None  \n",
    "            stop_extraction = False\n",
    "\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                    if stop_extraction:\n",
    "                        break  # Exit the loop if stop_extraction is set\n",
    "            \n",
    "                    # Extract data from the current page\n",
    "                    paragraphs, stop_extraction, rating, price = extract_text_with_format(page, provider=provider)\n",
    "                    \n",
    "                    # Append paragraphs from the current page\n",
    "                    all_paragraphs.extend(paragraphs)\n",
    "\n",
    "                    # Capture the first non-None rating and price\n",
    "                    if rating is not None and first_rating is None:\n",
    "                        first_rating = rating\n",
    "                    if price is not None and first_price is None:\n",
    "                        first_price = price\n",
    "                        \n",
    "            # Add extracted data to the list\n",
    "            data.append({\n",
    "                \"document_id\": id_counter,  # Unique ID\n",
    "                \"filename\": filename,\n",
    "                \"date\": date,\n",
    "                \"provider\": provider,\n",
    "                \"ticker\": ticker,\n",
    "                \"company_name\": company_name,\n",
    "                \"industry\": industry,\n",
    "                \"paragraphs\": all_paragraphs,\n",
    "                \"target_price\":first_price,\n",
    "                \"rating\": first_rating.lower() if first_rating else None\n",
    "                })\n",
    "            id_counter += 1  # Increment the ID counter for the next row\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Directory path\n",
    "pdf_directory = \"/Users/oskarroeske/Desktop/Analyst_Reports/Staging\"  # Replace with your actual folder path\n",
    "\n",
    "# Run the function and store results in a DataFrame\n",
    "df_reports = extract_text_from_all_pdfs_to_dataframe(pdf_directory, provider_info, ticker_map)\n",
    "\n",
    "# Display the resulting DataFrame to confirm\n",
    "df_reports.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_paragraph(paragraph):\n",
    "    # Remove email addresses\n",
    "    paragraph = re.sub(r\"\\S+@\\S+\", \"\", paragraph)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    paragraph = re.sub(r\"\\b\\d{1,3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\", \"\", paragraph)\n",
    "    \n",
    "    # Remove URLs\n",
    "    paragraph = re.sub(r\"http\\S+|www\\S+\", \"\", paragraph)\n",
    "    \n",
    "    # Remove special characters (keep alphanumerics, spaces, and common punctuation)\n",
    "    paragraph = re.sub(r\"[^\\w\\s,.!?]\", \"\", paragraph)\n",
    "    \n",
    "    # Remove multiple spaces or newlines\n",
    "    paragraph = re.sub(r\"\\s+\", \" \", paragraph).strip()\n",
    "    \n",
    "    # Remove boilerplate phrases\n",
    "    boilerplate_phrases = [\"Disclaimer\", \"Confidential\", \"For internal use only\"]\n",
    "    for phrase in boilerplate_phrases:\n",
    "        paragraph = paragraph.replace(phrase, \"\")\n",
    "    \n",
    "    # Remove short sentences (fewer than 10 words)\n",
    "    if len(paragraph.split()) < 6:\n",
    "        return None\n",
    "    \n",
    "    return paragraph\n",
    "\n",
    "# Apply cleaning function\n",
    "df_reports['paragraphs'] = df_reports['paragraphs'].apply(\n",
    "    lambda paragraphs: [clean_paragraph(p) for p in paragraphs if clean_paragraph(p)]\n",
    ")\n",
    "\n",
    "# Drop rows where the `paragraphs` column is empty after cleaning\n",
    "df_reports = df_reports[df_reports['paragraphs'].str.len() > 0]\n",
    "\n",
    "df_reports[\"rating\"] = df_reports[\"rating\"].astype(str).apply(lambda x: x.lower() if x != \"nan\" else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Date and Price for Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6 entries, 0 to 7\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   document_id   6 non-null      int64         \n",
      " 1   filename      6 non-null      object        \n",
      " 2   date          6 non-null      datetime64[ns]\n",
      " 3   provider      6 non-null      object        \n",
      " 4   ticker        6 non-null      object        \n",
      " 5   company_name  6 non-null      object        \n",
      " 6   industry      6 non-null      object        \n",
      " 7   paragraphs    6 non-null      object        \n",
      " 8   target_price  3 non-null      object        \n",
      " 9   rating        6 non-null      object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(8)\n",
      "memory usage: 528.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_reports.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and ensure 'Date' is in datetime format\n",
    "df_performance_data = pd.read_csv(\"performance_data.csv\")\n",
    "\n",
    "# Convert 'Date' column to datetime and set it as index (with timezone awareness)\n",
    "df_performance_data['Date'] = pd.to_datetime(df_performance_data['Date'], utc=True)\n",
    "df_performance_data = df_performance_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports = df_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prices(ticker, start_date, end_date=None):\n",
    "    try:\n",
    "        # Ensure start_date and end_date are in the same timezone (UTC)\n",
    "        start_date = pd.to_datetime(start_date, utc=True)\n",
    "        if end_date is not None:\n",
    "            end_date = pd.to_datetime(end_date, utc=True)\n",
    "        else:\n",
    "            end_date = start_date  # If no end_date, use start_date for a single day\n",
    "\n",
    "        # Filter the data for the ticker and date range\n",
    "        filtered_data = df_performance_data.loc[\n",
    "            (df_performance_data.index >= start_date) & \n",
    "            (df_performance_data.index <= end_date), ticker]\n",
    "\n",
    "        if not filtered_data.empty:\n",
    "            # Return the maximum price within the filtered date range\n",
    "            max_price = filtered_data.max()\n",
    "            min_price = filtered_data.min()\n",
    "            return float(max_price), float(min_price)\n",
    "        else:\n",
    "            # If no data available in the range, find the next available date using asof()\n",
    "            next_available_data = df_performance_data[ticker].asof(start_date)\n",
    "            if next_available_data is not None:\n",
    "                return float(next_available_data), float(next_available_data)\n",
    "            else:\n",
    "                return float('nan'), float('nan')  # Return NaN for missing data\n",
    "\n",
    "    except KeyError:\n",
    "        return float('nan'), float('nan')  # Return NaN for missing data\n",
    "    except Exception as e:\n",
    "        return float('nan'), float('nan')  # Return NaN for missing data\n",
    "\n",
    "def calculate_prices(row):\n",
    "    short_name = row['ticker']\n",
    "    base_date = pd.to_datetime(row['date'], utc=True)\n",
    "\n",
    "    # Calculate prices\n",
    "    row['start price'] = get_stock_prices(short_name, base_date)[0]\n",
    "    row['one day after'] = get_stock_prices(short_name, base_date + pd.DateOffset(days=1))[0]\n",
    "    row['max price after 3 months'], row[\"min price after 3 months\"] = get_stock_prices(\n",
    "        short_name, base_date, base_date + pd.DateOffset(months=3)\n",
    "    )\n",
    "    row['max price after 6 months'], row['min price after 6 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=3), base_date + pd.DateOffset(months=6)\n",
    "    )\n",
    "    row['max price after 9 months'], row['min price after 9 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=6), base_date + pd.DateOffset(months=9)\n",
    "    )\n",
    "    row['max price after 12 months'], row['min price after 12 months'] = get_stock_prices(\n",
    "        short_name, base_date + pd.DateOffset(months=9), base_date + pd.DateOffset(months=12)\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "df_saved_reports = df_saved_reports.apply(calculate_prices, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>provider</th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>target_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>start price</th>\n",
       "      <th>one day after</th>\n",
       "      <th>max price after 3 months</th>\n",
       "      <th>min price after 3 months</th>\n",
       "      <th>max price after 6 months</th>\n",
       "      <th>min price after 6 months</th>\n",
       "      <th>max price after 9 months</th>\n",
       "      <th>min price after 9 months</th>\n",
       "      <th>max price after 12 months</th>\n",
       "      <th>min price after 12 months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20150305_Needham_CRM_CRM-_Not_Even_FX_Could_Ta...</td>\n",
       "      <td>2015-03-05</td>\n",
       "      <td>Needham</td>\n",
       "      <td>CRM</td>\n",
       "      <td>Salesforce.com Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>[Q4 was solid, highlighted by 32 billings grow...</td>\n",
       "      <td>80.00</td>\n",
       "      <td>buy</td>\n",
       "      <td>65.300110</td>\n",
       "      <td>64.274658</td>\n",
       "      <td>74.678482</td>\n",
       "      <td>63.398548</td>\n",
       "      <td>75.375381</td>\n",
       "      <td>64.881958</td>\n",
       "      <td>81.776962</td>\n",
       "      <td>68.077774</td>\n",
       "      <td>80.721649</td>\n",
       "      <td>53.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20160510_Phillip_Securities_AAPL_Apple_Inc._Th...</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>Phillip Securities</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>[Apple Inc. Snapshot Apple Inc. AAPL recently ...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>21.402905</td>\n",
       "      <td>21.194420</td>\n",
       "      <td>25.063862</td>\n",
       "      <td>20.697264</td>\n",
       "      <td>27.238321</td>\n",
       "      <td>23.755497</td>\n",
       "      <td>30.791845</td>\n",
       "      <td>24.474806</td>\n",
       "      <td>35.807560</td>\n",
       "      <td>30.722088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20170322_BTIG_NKTR_Nektar_Therapeutics.pdf</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>BTIG</td>\n",
       "      <td>NKTR</td>\n",
       "      <td>Nektar Therapeutics</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[Nektar announced positive results from the Ph...</td>\n",
       "      <td>22.00</td>\n",
       "      <td>buy</td>\n",
       "      <td>22.889999</td>\n",
       "      <td>22.530001</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>23.230000</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>57.990002</td>\n",
       "      <td>22.260000</td>\n",
       "      <td>108.440002</td>\n",
       "      <td>57.400002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20180504_Needham_AMZN_Momentum_Remains_High_wi...</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>Needham</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc.</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>[Amazon posted strong Q1 results with revenue ...</td>\n",
       "      <td>1,900.00</td>\n",
       "      <td>buy</td>\n",
       "      <td>79.047501</td>\n",
       "      <td>79.047501</td>\n",
       "      <td>93.180496</td>\n",
       "      <td>78.718498</td>\n",
       "      <td>101.975502</td>\n",
       "      <td>76.521004</td>\n",
       "      <td>88.617996</td>\n",
       "      <td>67.197998</td>\n",
       "      <td>98.123001</td>\n",
       "      <td>79.411003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>20181114_BTIG_PRGO_Perrigo_Company_plc.pdf</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>BTIG</td>\n",
       "      <td>PRGO</td>\n",
       "      <td>Perrigo</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[212 5273505 We have lowered our CY18 revenue ...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>54.523006</td>\n",
       "      <td>54.565964</td>\n",
       "      <td>55.247051</td>\n",
       "      <td>31.454027</td>\n",
       "      <td>45.529312</td>\n",
       "      <td>40.234013</td>\n",
       "      <td>47.905579</td>\n",
       "      <td>36.526798</td>\n",
       "      <td>49.510155</td>\n",
       "      <td>39.491028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>20151223_Needham_TSLA_SCTY-_SCTY_Analyst_Day_H...</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>Needham</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla Inc.</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>[INVESTMENT HIGHLIGHTS NA SCTY hosted a wellat...</td>\n",
       "      <td>None</td>\n",
       "      <td>hold</td>\n",
       "      <td>15.313333</td>\n",
       "      <td>15.371333</td>\n",
       "      <td>16.000668</td>\n",
       "      <td>9.578000</td>\n",
       "      <td>17.694668</td>\n",
       "      <td>13.093333</td>\n",
       "      <td>15.652667</td>\n",
       "      <td>12.876667</td>\n",
       "      <td>14.246667</td>\n",
       "      <td>12.096667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                           filename       date  \\\n",
       "0            1  20150305_Needham_CRM_CRM-_Not_Even_FX_Could_Ta... 2015-03-05   \n",
       "2            3  20160510_Phillip_Securities_AAPL_Apple_Inc._Th... 2016-05-10   \n",
       "3            4         20170322_BTIG_NKTR_Nektar_Therapeutics.pdf 2017-03-22   \n",
       "4            5  20180504_Needham_AMZN_Momentum_Remains_High_wi... 2018-05-04   \n",
       "5            6         20181114_BTIG_PRGO_Perrigo_Company_plc.pdf 2018-11-14   \n",
       "7            8  20151223_Needham_TSLA_SCTY-_SCTY_Analyst_Day_H... 2015-12-23   \n",
       "\n",
       "             provider ticker         company_name                industry  \\\n",
       "0             Needham    CRM  Salesforce.com Inc.              Technology   \n",
       "2  Phillip Securities   AAPL           Apple Inc.              Technology   \n",
       "3                BTIG   NKTR  Nektar Therapeutics              Healthcare   \n",
       "4             Needham   AMZN      Amazon.com Inc.  Consumer Discretionary   \n",
       "5                BTIG   PRGO              Perrigo              Healthcare   \n",
       "7             Needham   TSLA           Tesla Inc.              Automobile   \n",
       "\n",
       "                                          paragraphs target_price rating  \\\n",
       "0  [Q4 was solid, highlighted by 32 billings grow...        80.00    buy   \n",
       "2  [Apple Inc. Snapshot Apple Inc. AAPL recently ...         None   none   \n",
       "3  [Nektar announced positive results from the Ph...        22.00    buy   \n",
       "4  [Amazon posted strong Q1 results with revenue ...     1,900.00    buy   \n",
       "5  [212 5273505 We have lowered our CY18 revenue ...         None   none   \n",
       "7  [INVESTMENT HIGHLIGHTS NA SCTY hosted a wellat...         None   hold   \n",
       "\n",
       "   start price  one day after  max price after 3 months  \\\n",
       "0    65.300110      64.274658                 74.678482   \n",
       "2    21.402905      21.194420                 25.063862   \n",
       "3    22.889999      22.530001                 24.200001   \n",
       "4    79.047501      79.047501                 93.180496   \n",
       "5    54.523006      54.565964                 55.247051   \n",
       "7    15.313333      15.371333                 16.000668   \n",
       "\n",
       "   min price after 3 months  max price after 6 months  \\\n",
       "0                 63.398548                 75.375381   \n",
       "2                 20.697264                 27.238321   \n",
       "3                 17.540001                 23.230000   \n",
       "4                 78.718498                101.975502   \n",
       "5                 31.454027                 45.529312   \n",
       "7                  9.578000                 17.694668   \n",
       "\n",
       "   min price after 6 months  max price after 9 months  \\\n",
       "0                 64.881958                 81.776962   \n",
       "2                 23.755497                 30.791845   \n",
       "3                 17.790001                 57.990002   \n",
       "4                 76.521004                 88.617996   \n",
       "5                 40.234013                 47.905579   \n",
       "7                 13.093333                 15.652667   \n",
       "\n",
       "   min price after 9 months  max price after 12 months  \\\n",
       "0                 68.077774                  80.721649   \n",
       "2                 24.474806                  35.807560   \n",
       "3                 22.260000                 108.440002   \n",
       "4                 67.197998                  98.123001   \n",
       "5                 36.526798                  49.510155   \n",
       "7                 12.876667                  14.246667   \n",
       "\n",
       "   min price after 12 months  \n",
       "0                  53.811111  \n",
       "2                  30.722088  \n",
       "3                  57.400002  \n",
       "4                  79.411003  \n",
       "5                  39.491028  \n",
       "7                  12.096667  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saved_reports.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved_reports[\"target_price\"] = (\n",
    "    df_saved_reports[\"target_price\"]\n",
    "    #Remove comma and transform to float for later operations\n",
    "    .replace(\",\", \"\", regex=True) \n",
    "    .astype(float)                 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate performance of Target Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the target_price is below the current price, we need to check if it was below the target_price\n",
    "def classifcy_performance(df):\n",
    "    if df[\"target_price\"] > df[\"start price\"]:\n",
    "        df[\"tp reached after 3 months\"] = df[\"target_price\"] <= df[\"max price after 3 months\"]\n",
    "        df[\"tp reached after 6 months\"] = df[\"target_price\"] <= df[\"max price after 6 months\"]\n",
    "        df[\"tp reached after 9 months\"] = df[\"target_price\"] <= df[\"max price after 9 months\"]\n",
    "        df[\"tp reached after 12 months\"] = df[\"target_price\"] <= df[\"max price after 12 months\"]\n",
    "    else:\n",
    "        df[\"tp reached after 3 months\"] = df[\"target_price\"] >= df[\"min price after 3 months\"]\n",
    "        df[\"tp reached after 6 months\"] = df[\"target_price\"] >= df[\"min price after 6 months\"]\n",
    "        df[\"tp reached after 9 months\"] = df[\"target_price\"] >= df[\"min price after 9 months\"]\n",
    "        df[\"tp reached after 12 months\"] = df[\"target_price\"] <= df[\"min price after 12 months\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "df_saved_reports = df_saved_reports.apply(classifcy_performance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean/Filter Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "buy     2\n",
       "none    2\n",
       "hold    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saved_reports.value_counts(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_saved_reports.to_csv(\"/Users/oskarroeske/Masterthesis/full_analysis/preprocessed_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the cleaned list into separate rows\n",
    "df_exploded = df_saved_reports.explode('paragraphs').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_exploded[[\"document_id\",\"paragraphs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2h/tyzkflxs465698k6z8l3ktj00000gn/T/ipykernel_99240/3036036435.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_exploded['paragraph_id'] = df_exploded.groupby('document_id').cumcount() + 1\n"
     ]
    }
   ],
   "source": [
    "df_exploded['paragraph_id'] = df_exploded.groupby('document_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraph_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Q4 was solid, highlighted by 32 billings growt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>While F16 rev guide was only tweaked up mildly...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Other highlights included a solid top line per...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Q4 revenue was up 26 yy to 1.44B and in line w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Management was upbeat on the call, especially ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>We appreciate mgmt for offering more simplifie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>We were impressed by the demo of the Silevo pi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7</td>\n",
       "      <td>New subsidy programs or new states where Solar...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7</td>\n",
       "      <td>Leadership position could become a greater bar...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>Incurs costs of removing a solar system if eit...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                         paragraphs  \\\n",
       "0             1  Q4 was solid, highlighted by 32 billings growt...   \n",
       "1             1  While F16 rev guide was only tweaked up mildly...   \n",
       "2             1  Other highlights included a solid top line per...   \n",
       "3             1  Q4 revenue was up 26 yy to 1.44B and in line w...   \n",
       "4             1  Management was upbeat on the call, especially ...   \n",
       "..          ...                                                ...   \n",
       "78            7  We appreciate mgmt for offering more simplifie...   \n",
       "79            7  We were impressed by the demo of the Silevo pi...   \n",
       "80            7  New subsidy programs or new states where Solar...   \n",
       "81            7  Leadership position could become a greater bar...   \n",
       "82            7  Incurs costs of removing a solar system if eit...   \n",
       "\n",
       "    paragraph_id  \n",
       "0              1  \n",
       "1              2  \n",
       "2              3  \n",
       "3              4  \n",
       "4              5  \n",
       "..           ...  \n",
       "78             5  \n",
       "79             6  \n",
       "80             7  \n",
       "81             8  \n",
       "82             9  \n",
       "\n",
       "[83 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_exploded.to_csv(\"/Users/oskarroeske/Masterthesis/full_analysis/preprocessed_paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraph_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Q4 was solid, highlighted by 32 billings growt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>While F16 rev guide was only tweaked up mildly...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Other highlights included a solid top line per...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Q4 revenue was up 26 yy to 1.44B and in line w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Management was upbeat on the call, especially ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>We appreciate mgmt for offering more simplifie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>We were impressed by the demo of the Silevo pi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7</td>\n",
       "      <td>New subsidy programs or new states where Solar...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7</td>\n",
       "      <td>Leadership position could become a greater bar...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>Incurs costs of removing a solar system if eit...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                         paragraphs  \\\n",
       "0             1  Q4 was solid, highlighted by 32 billings growt...   \n",
       "1             1  While F16 rev guide was only tweaked up mildly...   \n",
       "2             1  Other highlights included a solid top line per...   \n",
       "3             1  Q4 revenue was up 26 yy to 1.44B and in line w...   \n",
       "4             1  Management was upbeat on the call, especially ...   \n",
       "..          ...                                                ...   \n",
       "78            7  We appreciate mgmt for offering more simplifie...   \n",
       "79            7  We were impressed by the demo of the Silevo pi...   \n",
       "80            7  New subsidy programs or new states where Solar...   \n",
       "81            7  Leadership position could become a greater bar...   \n",
       "82            7  Incurs costs of removing a solar system if eit...   \n",
       "\n",
       "    paragraph_id  \n",
       "0              1  \n",
       "1              2  \n",
       "2              3  \n",
       "3              4  \n",
       "4              5  \n",
       "..           ...  \n",
       "78             5  \n",
       "79             6  \n",
       "80             7  \n",
       "81             8  \n",
       "82             9  \n",
       "\n",
       "[83 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Prepare an empty list to store the results\n",
    "result = []\n",
    "\n",
    "# Function to split text into sentences using spaCy\n",
    "def split_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# Split each paragraph into sentences\n",
    "for _, row in df_exploded.iterrows():\n",
    "    doc_id = row['document_id']\n",
    "    paragraph_id = row['paragraph_id']\n",
    "    text = row['paragraphs']\n",
    "    sentences = split_into_sentences(text)\n",
    "    for sentence_id, sentence in enumerate(sentences, start=1):\n",
    "        # Check if the sentence has more than 3 words\n",
    "        if len(sentence.split()) >= 3:\n",
    "            result.append({\n",
    "                'document_id': doc_id,\n",
    "                'paragraph_id': paragraph_id,\n",
    "                'sentence_id': sentence_id,\n",
    "                'sentence': sentence\n",
    "            })\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "df_sentences = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q4 was solid, highlighted by 32 billings growt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>One of the highlights on the call was the earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Another standout on the call was the number of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>While F16 rev guide was only tweaked up mildly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Encouragingly, management appeared committed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>We were impressed by the demo of the Silevo pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>We believe these technologies will help SCTY r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>New subsidy programs or new states where Solar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Leadership position could become a greater bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Incurs costs of removing a solar system if eit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id  paragraph_id  sentence_id  \\\n",
       "0              1             1            1   \n",
       "1              1             1            2   \n",
       "2              1             1            3   \n",
       "3              1             2            1   \n",
       "4              1             2            2   \n",
       "..           ...           ...          ...   \n",
       "255            7             6            1   \n",
       "256            7             6            2   \n",
       "257            7             7            1   \n",
       "258            7             8            1   \n",
       "259            7             9            1   \n",
       "\n",
       "                                              sentence  \n",
       "0    Q4 was solid, highlighted by 32 billings growt...  \n",
       "1    One of the highlights on the call was the earl...  \n",
       "2    Another standout on the call was the number of...  \n",
       "3    While F16 rev guide was only tweaked up mildly...  \n",
       "4    Encouragingly, management appeared committed t...  \n",
       "..                                                 ...  \n",
       "255  We were impressed by the demo of the Silevo pi...  \n",
       "256  We believe these technologies will help SCTY r...  \n",
       "257  New subsidy programs or new states where Solar...  \n",
       "258  Leadership position could become a greater bar...  \n",
       "259  Incurs costs of removing a solar system if eit...  \n",
       "\n",
       "[260 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.to_csv(\"/Users/oskarroeske/Masterthesis/full_analysis/preprocessed_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reports.to_csv(\"paragraphs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
